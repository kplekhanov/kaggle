{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4d39a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pressure = 64.8209917386395 Min pressure = -1.895744294564641\n",
      "The first 25 unique pressures...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train_gf = pd.read_csv('./data/train.csv')\n",
    "\n",
    "plt.title('Histogram of Train Pressures', size=14)\n",
    "plt.hist(train_gf.sample(100_000).pressure, bins=100)\n",
    "plt.show()\n",
    "print('Max pressure =',train_gf.pressure.max(), 'Min pressure =',train_gf.pressure.min())\n",
    "\n",
    "all_pressure = np.sort( train_gf.pressure.unique() )\n",
    "print('The first 25 unique pressures...')\n",
    "PRESSURE_MIN = all_pressure[0].item()\n",
    "PRESSURE_MAX = all_pressure[-1].item()\n",
    "PRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce5d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/274717\n",
    "\n",
    "import tensorflow as tf, gc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import RobustScaler, normalize\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fb3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "TRAIN_MODEL = False\n",
    "FOLDS = [0, 1, 2, 3, 4]\n",
    "EPOCH = 200\n",
    "BATCH_SIZE = 256\n",
    "if DEBUG:\n",
    "    EPOCH = 2\n",
    "    FOLDS = [0, 1]\n",
    "NUM_FOLDS = len(FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d239d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "if DEBUG:\n",
    "    df_train = df_train[:80*1000]\n",
    "\n",
    "def add_features(df):\n",
    "    df[\"dt\"] = df.groupby('breath_id')['time_step'].shift(-1) - df['time_step']\n",
    "    df[\"dt\"].fillna((df[\"dt\"].mean()), inplace=True)\n",
    "    \n",
    "    df['area'] = df['dt'] * df['u_in']\n",
    "    df['area_p1'] = df.groupby('breath_id')['area'].shift(1)\n",
    "    #df['area_p2'] = df.groupby('breath_id')['area'].shift(2)\n",
    "    df['area_m1'] = df.groupby('breath_id')['area'].shift(-1)\n",
    "    #df['area_m2'] = df.groupby('breath_id')['area'].shift(-2)\n",
    "    df['area_tot'] = df.groupby('breath_id')['area'].cumsum()\n",
    "    df['area_tot_p1'] = df.groupby('breath_id')['area_tot'].shift(1)\n",
    "    #df['area_tot_p2'] = df.groupby('breath_id')['area_tot'].shift(2)\n",
    "    df['area_tot_m1'] = df.groupby('breath_id')['area_tot'].shift(-1)\n",
    "    #df['area_tot_m2'] = df.groupby('breath_id')['area_tot'].shift(-2)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    #df['area_weird'] = df['time_step'] * df['u_in']\n",
    "    #df['area_weird'] = df.groupby('breath_id')['area_weird'].cumsum()\n",
    "    df['u_in_cumsum'] = df.groupby('breath_id')['u_in'].cumsum()\n",
    "    #df['breath_id__u_in__max'] = df.groupby('breath_id')['u_in'].transform('max')\n",
    "    #df['breath_id__u_in__mean'] = df.groupby('breath_id')['u_in'].transform('mean')\n",
    "    df['breath_id__u_in__diffmax'] = df.groupby('breath_id')['u_in'].transform('max') - df['u_in']\n",
    "    df['breath_id__u_in__diffmean'] = df.groupby('breath_id')['u_in'].transform('mean') - df['u_in']\n",
    "    \n",
    "    df['u_in_p1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
    "    #df['u_in_p1'].fillna(method='bfill', inplace=True)\n",
    "    df['u_in_p2'] = df.groupby('breath_id')['u_in_p1'].shift(1)\n",
    "    #df['u_in_p2'].fillna(method='bfill', inplace=True)\n",
    "    #df['u_in_p3'] = df.groupby('breath_id')['u_in_p2'].shift(1)\n",
    "    #df['u_in_p3'].fillna(method='bfill', inplace=True)\n",
    "    #df['u_in_p4'] = df.groupby('breath_id')['u_in_p3'].shift(1)\n",
    "    #df['u_in_p4'].fillna(method='bfill', inplace=True)\n",
    "    df['u_in_m1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
    "    df['u_in_m2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
    "    #df['u_in_m3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
    "    #df['u_in_m4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    df['u_in_diff1'] = df['u_in_m1'] - df['u_in']\n",
    "    df['u_in_diff1_p1'] = df.groupby('breath_id')['u_in_diff1'].shift(1).fillna(0)\n",
    "    #df['u_in_diff1_p2'] = df.groupby('breath_id')['u_in_diff1'].shift(2).fillna(0)\n",
    "    df['u_in_diff1_m1'] = df.groupby('breath_id')['u_in_diff1'].shift(-1).fillna(0)\n",
    "    #df['u_in_diff1_m2'] = df.groupby('breath_id')['u_in_diff1'].shift(-2).fillna(0)\n",
    "    #df['u_in_diff2'] = df['u_in_m2'] - df['u_in']\n",
    "    #df['u_in_diff3'] = df['u_in_m3'] - df['u_in']\n",
    "    #df['u_in_diff4'] = df['u_in_m4'] - df['u_in']\n",
    "    #df['u_in_derr1'] = df['u_in_diff1'] / df[\"dt\"]\n",
    "    \n",
    "    #df['R'] = df['R'].astype(str)\n",
    "    #df['C'] = df['C'].astype(str)\n",
    "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
    "    df.drop(['R', 'C'], axis=1, inplace=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    return df\n",
    "\n",
    "df_train = add_features(df_train)\n",
    "df_test = add_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bb51dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "      <th>dt</th>\n",
       "      <th>area</th>\n",
       "      <th>area_p1</th>\n",
       "      <th>area_m1</th>\n",
       "      <th>area_tot</th>\n",
       "      <th>area_tot_p1</th>\n",
       "      <th>area_tot_m1</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "      <th>breath_id__u_in__diffmax</th>\n",
       "      <th>breath_id__u_in__diffmean</th>\n",
       "      <th>u_in_p1</th>\n",
       "      <th>u_in_p2</th>\n",
       "      <th>u_in_m1</th>\n",
       "      <th>u_in_m2</th>\n",
       "      <th>u_in_diff1</th>\n",
       "      <th>u_in_diff1_p1</th>\n",
       "      <th>u_in_diff1_m1</th>\n",
       "      <th>R__C_20__10</th>\n",
       "      <th>R__C_20__20</th>\n",
       "      <th>R__C_20__50</th>\n",
       "      <th>R__C_50__10</th>\n",
       "      <th>R__C_50__20</th>\n",
       "      <th>R__C_50__50</th>\n",
       "      <th>R__C_5__10</th>\n",
       "      <th>R__C_5__20</th>\n",
       "      <th>R__C_5__50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>28.229702</td>\n",
       "      <td>10.062673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.299707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>18.466375</td>\n",
       "      <td>9.929994</td>\n",
       "      <td>-8.237035</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>18.299707</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>40.975653</td>\n",
       "      <td>5.803758</td>\n",
       "      <td>-12.363271</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>3.032234</td>\n",
       "      <td>63.784476</td>\n",
       "      <td>5.504214</td>\n",
       "      <td>-12.662816</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>27.259866</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>1.904016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "      <td>0.033942</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>0.927113</td>\n",
       "      <td>3.032234</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>3.959346</td>\n",
       "      <td>89.140326</td>\n",
       "      <td>2.957185</td>\n",
       "      <td>-15.209844</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>27.259866</td>\n",
       "      <td>27.127486</td>\n",
       "      <td>1.904016</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>-0.132380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  time_step       u_in  u_out   pressure        dt      area   area_p1   area_m1  area_tot  area_tot_p1  area_tot_m1  u_in_cumsum  breath_id__u_in__diffmax  breath_id__u_in__diffmean    u_in_p1    u_in_p2    u_in_m1    u_in_m2  u_in_diff1  u_in_diff1_p1  u_in_diff1_m1  R__C_20__10  R__C_20__20  R__C_20__50  R__C_50__10  R__C_50__20  R__C_50__50  R__C_5__10  R__C_5__20  R__C_5__50\n",
       "0   1          1   0.000000   0.083334      0   5.837492  0.033652  0.002804  0.000000  0.622489  0.002804     0.000000     0.625293     0.083334                 28.229702                  10.062673   0.000000   0.000000  18.383041  22.509278   18.299707       0.000000       4.126236            0            0            1            0            0            0           0           0           0\n",
       "1   2          1   0.033652  18.383041      0   5.907794  0.033862  0.622489  0.002804  0.765942  0.625293     0.002804     1.391235    18.466375                  9.929994                  -8.237035   0.083334   0.000000  22.509278  22.808822    4.126236      18.299707       0.299544            0            0            1            0            0            0           0           0           0\n",
       "2   3          1   0.067514  22.509278      0   7.876254  0.034028  0.765942  0.622489  0.780365  1.391235     0.625293     2.171600    40.975653                  5.803758                 -12.363271  18.383041   0.083334  22.808822  25.355850    0.299544       4.126236       2.547028            0            0            1            0            0            0           0           0           0\n",
       "3   4          1   0.101542  22.808822      0  11.742872  0.034213  0.780365  0.765942  0.860634  2.171600     1.391235     3.032234    63.784476                  5.504214                 -12.662816  22.509278  18.383041  25.355850  27.259866    2.547028       0.299544       1.904016            0            0            1            0            0            0           0           0           0\n",
       "4   5          1   0.135756  25.355850      0  12.234987  0.033942  0.860634  0.780365  0.927113  3.032234     2.171600     3.959346    89.140326                  2.957185                 -15.209844  22.808822  22.509278  27.259866  27.127486    1.904016       2.547028      -0.132380            0            0            1            0            0            0           0           0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b111abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df_train[['pressure']].to_numpy().reshape(-1, 80)\n",
    "u_outs = df_train[['u_out']].to_numpy().reshape(-1, 80)\n",
    "df_train.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\n",
    "df_test.drop(['id', 'breath_id'], axis=1, inplace=True)\n",
    "\n",
    "RS = RobustScaler()\n",
    "train = RS.fit_transform(df_train)\n",
    "test = RS.transform(df_test)\n",
    "\n",
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "test = test.reshape(-1, 80, train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1181ea6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>dt</th>\n",
       "      <th>area</th>\n",
       "      <th>area_p1</th>\n",
       "      <th>area_m1</th>\n",
       "      <th>area_tot</th>\n",
       "      <th>area_tot_p1</th>\n",
       "      <th>area_tot_m1</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "      <th>breath_id__u_in__diffmax</th>\n",
       "      <th>breath_id__u_in__diffmean</th>\n",
       "      <th>u_in_p1</th>\n",
       "      <th>u_in_p2</th>\n",
       "      <th>u_in_m1</th>\n",
       "      <th>u_in_m2</th>\n",
       "      <th>u_in_diff1</th>\n",
       "      <th>u_in_diff1_p1</th>\n",
       "      <th>u_in_diff1_m1</th>\n",
       "      <th>R__C_20__10</th>\n",
       "      <th>R__C_20__20</th>\n",
       "      <th>R__C_20__50</th>\n",
       "      <th>R__C_50__10</th>\n",
       "      <th>R__C_50__20</th>\n",
       "      <th>R__C_50__50</th>\n",
       "      <th>R__C_5__10</th>\n",
       "      <th>R__C_5__20</th>\n",
       "      <th>R__C_5__50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>28.229702</td>\n",
       "      <td>10.062673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.299707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>18.466375</td>\n",
       "      <td>9.929994</td>\n",
       "      <td>-8.237035</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>18.299707</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.622489</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>0.625293</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>40.975653</td>\n",
       "      <td>5.803758</td>\n",
       "      <td>-12.363271</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>0.765942</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>1.391235</td>\n",
       "      <td>3.032234</td>\n",
       "      <td>63.784476</td>\n",
       "      <td>5.504214</td>\n",
       "      <td>-12.662816</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>27.259866</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>1.904016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033942</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>0.780365</td>\n",
       "      <td>0.927113</td>\n",
       "      <td>3.032234</td>\n",
       "      <td>2.171600</td>\n",
       "      <td>3.959346</td>\n",
       "      <td>89.140326</td>\n",
       "      <td>2.957185</td>\n",
       "      <td>-15.209844</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>27.259866</td>\n",
       "      <td>27.127486</td>\n",
       "      <td>1.904016</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>-0.132380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_step       u_in  u_out        dt      area   area_p1   area_m1  area_tot  area_tot_p1  area_tot_m1  u_in_cumsum  breath_id__u_in__diffmax  breath_id__u_in__diffmean    u_in_p1    u_in_p2    u_in_m1    u_in_m2  u_in_diff1  u_in_diff1_p1  u_in_diff1_m1  R__C_20__10  R__C_20__20  R__C_20__50  R__C_50__10  R__C_50__20  R__C_50__50  R__C_5__10  R__C_5__20  R__C_5__50\n",
       "0   0.000000   0.083334      0  0.033652  0.002804  0.000000  0.622489  0.002804     0.000000     0.625293     0.083334                 28.229702                  10.062673   0.000000   0.000000  18.383041  22.509278   18.299707       0.000000       4.126236            0            0            1            0            0            0           0           0           0\n",
       "1   0.033652  18.383041      0  0.033862  0.622489  0.002804  0.765942  0.625293     0.002804     1.391235    18.466375                  9.929994                  -8.237035   0.083334   0.000000  22.509278  22.808822    4.126236      18.299707       0.299544            0            0            1            0            0            0           0           0           0\n",
       "2   0.067514  22.509278      0  0.034028  0.765942  0.622489  0.780365  1.391235     0.625293     2.171600    40.975653                  5.803758                 -12.363271  18.383041   0.083334  22.808822  25.355850    0.299544       4.126236       2.547028            0            0            1            0            0            0           0           0           0\n",
       "3   0.101542  22.808822      0  0.034213  0.780365  0.765942  0.860634  2.171600     1.391235     3.032234    63.784476                  5.504214                 -12.662816  22.509278  18.383041  25.355850  27.259866    2.547028       0.299544       1.904016            0            0            1            0            0            0           0           0           0\n",
       "4   0.135756  25.355850      0  0.033942  0.860634  0.780365  0.927113  3.032234     2.171600     3.959346    89.140326                  2.957185                 -15.209844  22.808822  22.509278  27.259866  27.127486    1.904016       2.547028      -0.132380            0            0            1            0            0            0           0           0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e95e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 0 < ---------------\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LayerNormLSTMCell.call of <tensorflow_addons.rnn.layer_norm_lstm_cell.LayerNormLSTMCell object at 0x7ff6014bdb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LayerNormLSTMCell.call of <tensorflow_addons.rnn.layer_norm_lstm_cell.LayerNormLSTMCell object at 0x7ff6014bdb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:AutoGraph could not transform <function GBVPP_loss at 0x7ff6b4623160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function GBVPP_loss at 0x7ff6b4623160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "236/236 [==============================] - 198s 803ms/step - loss: 3.7958 - val_loss: 1.0273\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02729, saving model to folds0.hdf5\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.9409 - val_loss: 0.7801\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02729 to 0.78009, saving model to folds0.hdf5\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.7616 - val_loss: 0.6866\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.78009 to 0.68663, saving model to folds0.hdf5\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.6629 - val_loss: 0.6322\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68663 to 0.63223, saving model to folds0.hdf5\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.7196 - val_loss: 0.5578\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63223 to 0.55776, saving model to folds0.hdf5\n",
      "Epoch 6/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.5825 - val_loss: 0.8084\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55776\n",
      "Epoch 7/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.6352 - val_loss: 0.5428\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.55776 to 0.54282, saving model to folds0.hdf5\n",
      "Epoch 8/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.5363 - val_loss: 0.5050\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.54282 to 0.50497, saving model to folds0.hdf5\n",
      "Epoch 9/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.5615 - val_loss: 0.5600\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50497\n",
      "Epoch 10/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.5316 - val_loss: 0.5260\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50497\n",
      "Epoch 11/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4789 - val_loss: 0.4948\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50497 to 0.49482, saving model to folds0.hdf5\n",
      "Epoch 12/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4767 - val_loss: 0.5173\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49482\n",
      "Epoch 13/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.4667 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.49482 to 0.43326, saving model to folds0.hdf5\n",
      "Epoch 14/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.4277 - val_loss: 0.4375\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43326\n",
      "Epoch 15/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.4187 - val_loss: 0.3966\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.43326 to 0.39658, saving model to folds0.hdf5\n",
      "Epoch 16/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.4020 - val_loss: 0.4105\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39658\n",
      "Epoch 17/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4071 - val_loss: 0.4533\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39658\n",
      "Epoch 18/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.3852 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39658\n",
      "Epoch 19/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.4261 - val_loss: 0.4737\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.39658\n",
      "Epoch 20/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.4185 - val_loss: 0.4063\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.39658\n",
      "Epoch 21/200\n",
      "236/236 [==============================] - 199s 845ms/step - loss: 0.3790 - val_loss: 0.4131\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.39658\n",
      "Epoch 22/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.4046 - val_loss: 0.3949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.39658 to 0.39489, saving model to folds0.hdf5\n",
      "Epoch 23/200\n",
      "236/236 [==============================] - 194s 823ms/step - loss: 0.3642 - val_loss: 0.3813\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.39489 to 0.38130, saving model to folds0.hdf5\n",
      "Epoch 24/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.3817 - val_loss: 0.3501\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.38130 to 0.35008, saving model to folds0.hdf5\n",
      "Epoch 25/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.3632 - val_loss: 0.3849\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.35008\n",
      "Epoch 26/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.3298 - val_loss: 0.3393\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.35008 to 0.33928, saving model to folds0.hdf5\n",
      "Epoch 27/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.3962 - val_loss: 0.3473\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.33928\n",
      "Epoch 28/200\n",
      "236/236 [==============================] - 197s 834ms/step - loss: 0.3491 - val_loss: 0.3072\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.33928 to 0.30720, saving model to folds0.hdf5\n",
      "Epoch 29/200\n",
      "236/236 [==============================] - 202s 856ms/step - loss: 0.3105 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30720\n",
      "Epoch 30/200\n",
      "236/236 [==============================] - 202s 857ms/step - loss: 0.3627 - val_loss: 0.4180\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.30720\n",
      "Epoch 31/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.3678 - val_loss: 0.3595\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.30720\n",
      "Epoch 32/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.3109 - val_loss: 0.3174\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30720\n",
      "Epoch 33/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.2907 - val_loss: 0.3007\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.30720 to 0.30073, saving model to folds0.hdf5\n",
      "Epoch 34/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.3030 - val_loss: 0.3246\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30073\n",
      "Epoch 35/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.2947 - val_loss: 0.2913\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.30073 to 0.29129, saving model to folds0.hdf5\n",
      "Epoch 36/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.2903 - val_loss: 0.3962\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.29129\n",
      "Epoch 37/200\n",
      "236/236 [==============================] - 197s 833ms/step - loss: 0.3439 - val_loss: 0.3360\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.29129\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 207s 876ms/step - loss: 0.3232 - val_loss: 0.3147\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.29129\n",
      "Epoch 39/200\n",
      "236/236 [==============================] - 199s 842ms/step - loss: 0.2991 - val_loss: 0.3249\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.29129\n",
      "Epoch 40/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.3054 - val_loss: 0.4454\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29129\n",
      "Epoch 41/200\n",
      "236/236 [==============================] - 192s 814ms/step - loss: 0.3705 - val_loss: 0.3448\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29129\n",
      "Epoch 42/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.3065 - val_loss: 0.3174\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.29129\n",
      "Epoch 43/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.2789 - val_loss: 0.2901\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.29129 to 0.29014, saving model to folds0.hdf5\n",
      "Epoch 44/200\n",
      "236/236 [==============================] - 209s 886ms/step - loss: 0.2865 - val_loss: 0.3876\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.29014\n",
      "Epoch 45/200\n",
      "236/236 [==============================] - 203s 861ms/step - loss: 0.2972 - val_loss: 0.2941\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.29014\n",
      "Epoch 46/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.2589 - val_loss: 0.2761\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.29014 to 0.27609, saving model to folds0.hdf5\n",
      "Epoch 47/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.2514 - val_loss: 0.2752\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.27609 to 0.27522, saving model to folds0.hdf5\n",
      "Epoch 48/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.2459 - val_loss: 0.2805\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.27522\n",
      "Epoch 49/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.2470 - val_loss: 0.2634\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.27522 to 0.26343, saving model to folds0.hdf5\n",
      "Epoch 50/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.2699 - val_loss: 0.2836\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.26343\n",
      "Epoch 51/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.2764 - val_loss: 0.3081\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.26343\n",
      "Epoch 52/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.3295 - val_loss: 0.3424\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.26343\n",
      "Epoch 53/200\n",
      "236/236 [==============================] - 205s 867ms/step - loss: 0.2744 - val_loss: 0.2898\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.26343\n",
      "Epoch 54/200\n",
      "236/236 [==============================] - 219s 926ms/step - loss: 0.2456 - val_loss: 0.3645\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.26343\n",
      "Epoch 55/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.2728 - val_loss: 0.2666\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.26343\n",
      "Epoch 56/200\n",
      "236/236 [==============================] - 191s 812ms/step - loss: 0.2246 - val_loss: 0.2488\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.26343 to 0.24880, saving model to folds0.hdf5\n",
      "Epoch 57/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.2100 - val_loss: 0.2500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24880\n",
      "Epoch 58/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2170 - val_loss: 0.2737\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24880\n",
      "Epoch 59/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.2379 - val_loss: 0.2775\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24880\n",
      "Epoch 60/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2383 - val_loss: 0.2719\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.24880\n",
      "Epoch 61/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.2296 - val_loss: 0.3714\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24880\n",
      "Epoch 62/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.2949 - val_loss: 0.2647\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24880\n",
      "Epoch 63/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.2153 - val_loss: 0.2614\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24880\n",
      "Epoch 64/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.2117 - val_loss: 0.2368\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.24880 to 0.23678, saving model to folds0.hdf5\n",
      "Epoch 65/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.2241 - val_loss: 0.2369\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.23678\n",
      "Epoch 66/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1993 - val_loss: 0.2350\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.23678 to 0.23498, saving model to folds0.hdf5\n",
      "Epoch 67/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.1987 - val_loss: 0.2746\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.23498\n",
      "Epoch 68/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.2084 - val_loss: 0.2637\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.23498\n",
      "Epoch 69/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2251 - val_loss: 0.2870\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.23498\n",
      "Epoch 70/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2316 - val_loss: 0.2849\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.23498\n",
      "Epoch 71/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.2133 - val_loss: 0.2730\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.23498\n",
      "Epoch 72/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2629 - val_loss: 0.2868\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.23498\n",
      "Epoch 73/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.2375 - val_loss: 0.3166\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.23498\n",
      "Epoch 74/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.2793 - val_loss: 0.2676\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.23498\n",
      "Epoch 75/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.2202 - val_loss: 0.2675\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.23498\n",
      "Epoch 76/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.2087 - val_loss: 0.2442\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.23498\n",
      "Epoch 77/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1692 - val_loss: 0.2210\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.23498 to 0.22098, saving model to folds0.hdf5\n",
      "Epoch 78/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1546 - val_loss: 0.2092\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.22098 to 0.20925, saving model to folds0.hdf5\n",
      "Epoch 79/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1474 - val_loss: 0.2031\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.20925 to 0.20314, saving model to folds0.hdf5\n",
      "Epoch 80/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.1468 - val_loss: 0.2026\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.20314 to 0.20263, saving model to folds0.hdf5\n",
      "Epoch 81/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.1474 - val_loss: 0.2068\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.20263\n",
      "Epoch 82/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1431 - val_loss: 0.2053\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.20263\n",
      "Epoch 83/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1411 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.20263\n",
      "Epoch 84/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1435 - val_loss: 0.2023\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.20263 to 0.20232, saving model to folds0.hdf5\n",
      "Epoch 85/200\n",
      "236/236 [==============================] - 199s 846ms/step - loss: 0.1444 - val_loss: 0.2076\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.20232\n",
      "Epoch 86/200\n",
      "236/236 [==============================] - 196s 830ms/step - loss: 0.1466 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.20232 to 0.20056, saving model to folds0.hdf5\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 190s 806ms/step - loss: 0.1368 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.20056\n",
      "Epoch 88/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.1374 - val_loss: 0.2082\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.20056\n",
      "Epoch 89/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.1539 - val_loss: 0.2049\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.20056\n",
      "Epoch 90/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.1456 - val_loss: 0.2075\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.20056\n",
      "Epoch 91/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.1436 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.20056 to 0.19927, saving model to folds0.hdf5\n",
      "Epoch 92/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.1301 - val_loss: 0.2028\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.19927\n",
      "Epoch 93/200\n",
      "236/236 [==============================] - 192s 816ms/step - loss: 0.1441 - val_loss: 0.2030\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.19927\n",
      "Epoch 94/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.1351 - val_loss: 0.2015\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.19927\n",
      "Epoch 95/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.1287 - val_loss: 0.1944\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.19927 to 0.19443, saving model to folds0.hdf5\n",
      "Epoch 96/200\n",
      "236/236 [==============================] - 192s 814ms/step - loss: 0.1242 - val_loss: 0.2032\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.19443\n",
      "Epoch 97/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.1408 - val_loss: 0.2307\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.19443\n",
      "Epoch 98/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.1383 - val_loss: 0.1982\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.19443\n",
      "Epoch 99/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.1253 - val_loss: 0.1937\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.19443 to 0.19366, saving model to folds0.hdf5\n",
      "Epoch 100/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.1288 - val_loss: 0.2011\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.19366\n",
      "Epoch 101/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1326 - val_loss: 0.1919\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.19366 to 0.19194, saving model to folds0.hdf5\n",
      "Epoch 102/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.1288 - val_loss: 0.2024\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.19194\n",
      "Epoch 103/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.1256 - val_loss: 0.1911\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.19194 to 0.19113, saving model to folds0.hdf5\n",
      "Epoch 104/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1193 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.19113\n",
      "Epoch 105/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.1201 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.19113 to 0.18602, saving model to folds0.hdf5\n",
      "Epoch 106/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.1151 - val_loss: 0.1887\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18602\n",
      "Epoch 107/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.1170 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.18602 to 0.18544, saving model to folds0.hdf5\n",
      "Epoch 108/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1149 - val_loss: 0.1871\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18544\n",
      "Epoch 109/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1152 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18544\n",
      "Epoch 110/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1117 - val_loss: 0.1913\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18544\n",
      "Epoch 111/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1163 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18544\n",
      "Epoch 112/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.1256 - val_loss: 0.1929\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.18544\n",
      "Epoch 113/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.1296 - val_loss: 0.2036\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18544\n",
      "Epoch 114/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1249 - val_loss: 0.2018\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18544\n",
      "Epoch 115/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.1232 - val_loss: 0.2048\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18544\n",
      "Epoch 116/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.1278 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18544\n",
      "Epoch 117/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.1337 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18544\n",
      "Epoch 118/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1041 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.18544 to 0.18107, saving model to folds0.hdf5\n",
      "Epoch 119/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0948 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.18107 to 0.17833, saving model to folds0.hdf5\n",
      "Epoch 120/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0932 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17833\n",
      "Epoch 121/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0935 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17833\n",
      "Epoch 122/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0929 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17833\n",
      "Epoch 123/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0925 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.17833 to 0.17796, saving model to folds0.hdf5\n",
      "Epoch 124/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0908 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.17796 to 0.17770, saving model to folds0.hdf5\n",
      "Epoch 125/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0906 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17770\n",
      "Epoch 126/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0913 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.17770 to 0.17707, saving model to folds0.hdf5\n",
      "Epoch 127/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0880 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17707\n",
      "Epoch 128/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0891 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.17707 to 0.17553, saving model to folds0.hdf5\n",
      "Epoch 129/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0873 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17553\n",
      "Epoch 130/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0884 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17553\n",
      "Epoch 131/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0876 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17553\n",
      "Epoch 132/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0866 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17553\n",
      "Epoch 133/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0853 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17553\n",
      "Epoch 134/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0858 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.17553 to 0.17501, saving model to folds0.hdf5\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0846 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17501\n",
      "Epoch 136/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0853 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.17501 to 0.17488, saving model to folds0.hdf5\n",
      "Epoch 137/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0846 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17488\n",
      "Epoch 138/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0878 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.17488 to 0.17470, saving model to folds0.hdf5\n",
      "Epoch 139/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0841 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17470\n",
      "Epoch 140/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0842 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.17470 to 0.17448, saving model to folds0.hdf5\n",
      "Epoch 141/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0827 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17448\n",
      "Epoch 142/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0835 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17448\n",
      "Epoch 143/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0813 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17448\n",
      "Epoch 144/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0831 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.17448 to 0.17402, saving model to folds0.hdf5\n",
      "Epoch 145/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0817 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17402\n",
      "Epoch 146/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0819 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17402\n",
      "Epoch 147/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0809 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17402\n",
      "Epoch 148/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0789 - val_loss: 0.1736\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.17402 to 0.17356, saving model to folds0.hdf5\n",
      "Epoch 149/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0798 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.17356 to 0.17289, saving model to folds0.hdf5\n",
      "Epoch 150/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0798 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17289\n",
      "Epoch 151/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0795 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17289\n",
      "Epoch 152/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0774 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17289\n",
      "Epoch 153/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0806 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17289\n",
      "Epoch 154/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0796 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17289\n",
      "Epoch 155/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0782 - val_loss: 0.1763\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17289\n",
      "Epoch 156/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0774 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17289\n",
      "Epoch 157/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0764 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17289\n",
      "Epoch 158/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0764 - val_loss: 0.1732\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17289\n",
      "Epoch 159/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0771 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17289\n",
      "Epoch 160/200\n",
      "236/236 [==============================] - 189s 798ms/step - loss: 0.0714 - val_loss: 0.1707\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.17289 to 0.17069, saving model to folds0.hdf5\n",
      "Epoch 161/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0679 - val_loss: 0.1708\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17069\n",
      "Epoch 162/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0671 - val_loss: 0.1711\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17069\n",
      "Epoch 163/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0668 - val_loss: 0.1703\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.17069 to 0.17028, saving model to folds0.hdf5\n",
      "Epoch 164/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0669 - val_loss: 0.1706\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17028\n",
      "Epoch 165/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0664 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.17028 to 0.17006, saving model to folds0.hdf5\n",
      "Epoch 166/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0668 - val_loss: 0.1699\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.17006 to 0.16986, saving model to folds0.hdf5\n",
      "Epoch 167/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0660 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.16986\n",
      "Epoch 168/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0662 - val_loss: 0.1706\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.16986\n",
      "Epoch 169/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0656 - val_loss: 0.1702\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.16986\n",
      "Epoch 170/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0656 - val_loss: 0.1710\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.16986\n",
      "Epoch 171/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0662 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.16986\n",
      "Epoch 172/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0661 - val_loss: 0.1716\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.16986\n",
      "Epoch 173/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0650 - val_loss: 0.1707\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.16986\n",
      "Epoch 174/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0648 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.16986 to 0.16955, saving model to folds0.hdf5\n",
      "Epoch 175/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0651 - val_loss: 0.1697\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.16955\n",
      "Epoch 176/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0643 - val_loss: 0.1707\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.16955\n",
      "Epoch 177/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0648 - val_loss: 0.1704\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.16955\n",
      "Epoch 178/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0647 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.16955\n",
      "Epoch 179/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0640 - val_loss: 0.1698\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.16955\n",
      "Epoch 180/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0643 - val_loss: 0.1703\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.16955\n",
      "Epoch 181/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0639 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.16955\n",
      "Epoch 182/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0633 - val_loss: 0.1691\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.16955 to 0.16915, saving model to folds0.hdf5\n",
      "Epoch 183/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0626 - val_loss: 0.1698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00183: val_loss did not improve from 0.16915\n",
      "Epoch 184/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.0637 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.16915\n",
      "Epoch 185/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0634 - val_loss: 0.1697\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.16915\n",
      "Epoch 186/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0625 - val_loss: 0.1704\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.16915\n",
      "Epoch 187/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0622 - val_loss: 0.1698\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.16915\n",
      "Epoch 188/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0624 - val_loss: 0.1696\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.16915\n",
      "Epoch 189/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0620 - val_loss: 0.1697\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.16915\n",
      "Epoch 190/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0609 - val_loss: 0.1700\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.16915\n",
      "Epoch 191/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0618 - val_loss: 0.1713\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.16915\n",
      "Epoch 192/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0633 - val_loss: 0.1698\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.16915\n",
      "Epoch 193/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0598 - val_loss: 0.1692\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.16915\n",
      "Epoch 194/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0574 - val_loss: 0.1686\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.16915 to 0.16863, saving model to folds0.hdf5\n",
      "Epoch 195/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0575 - val_loss: 0.1688\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.16863\n",
      "Epoch 196/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0568 - val_loss: 0.1687\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.16863\n",
      "Epoch 197/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0563 - val_loss: 0.1689\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.16863\n",
      "Epoch 198/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0569 - val_loss: 0.1687\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.16863\n",
      "Epoch 199/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0566 - val_loss: 0.1692\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.16863\n",
      "Epoch 200/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0572 - val_loss: 0.1687\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.16863\n",
      "197/197 - 31s\n",
      "--------------- > Fold 1 < ---------------\n",
      "Epoch 1/200\n",
      "236/236 [==============================] - 196s 797ms/step - loss: 4.0386 - val_loss: 1.0170\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01703, saving model to folds1.hdf5\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.9493 - val_loss: 0.8271\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01703 to 0.82711, saving model to folds1.hdf5\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.8725 - val_loss: 0.7866\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.82711 to 0.78656, saving model to folds1.hdf5\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.7162 - val_loss: 0.6758\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78656 to 0.67578, saving model to folds1.hdf5\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.6501 - val_loss: 0.6206\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67578 to 0.62063, saving model to folds1.hdf5\n",
      "Epoch 6/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.6488 - val_loss: 0.6235\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62063\n",
      "Epoch 7/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5931 - val_loss: 0.5274\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.62063 to 0.52740, saving model to folds1.hdf5\n",
      "Epoch 8/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.5352 - val_loss: 0.5738\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52740\n",
      "Epoch 9/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5686 - val_loss: 0.5405\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.52740\n",
      "Epoch 10/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.4913 - val_loss: 0.5040\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.52740 to 0.50399, saving model to folds1.hdf5\n",
      "Epoch 11/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.4730 - val_loss: 0.5639\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50399\n",
      "Epoch 12/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.4974 - val_loss: 0.5226\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50399\n",
      "Epoch 13/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4470 - val_loss: 0.4518\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50399 to 0.45183, saving model to folds1.hdf5\n",
      "Epoch 14/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.4595 - val_loss: 0.4811\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45183\n",
      "Epoch 15/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.4379 - val_loss: 0.4290\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45183 to 0.42896, saving model to folds1.hdf5\n",
      "Epoch 16/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4011 - val_loss: 0.4574\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.42896\n",
      "Epoch 17/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3968 - val_loss: 0.4517\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42896\n",
      "Epoch 18/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.4123 - val_loss: 0.4375\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42896\n",
      "Epoch 19/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3922 - val_loss: 0.4064\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.42896 to 0.40642, saving model to folds1.hdf5\n",
      "Epoch 20/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.3631 - val_loss: 0.4216\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.40642\n",
      "Epoch 21/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3691 - val_loss: 0.3899\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.40642 to 0.38988, saving model to folds1.hdf5\n",
      "Epoch 22/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3734 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.38988\n",
      "Epoch 23/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3539 - val_loss: 0.3773\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.38988 to 0.37731, saving model to folds1.hdf5\n",
      "Epoch 24/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3391 - val_loss: 0.3839\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.37731\n",
      "Epoch 25/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3430 - val_loss: 0.3683\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.37731 to 0.36833, saving model to folds1.hdf5\n",
      "Epoch 26/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3546 - val_loss: 0.5485\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.36833\n",
      "Epoch 27/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4567 - val_loss: 0.4075\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.36833\n",
      "Epoch 28/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3580 - val_loss: 0.3639\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.36833 to 0.36392, saving model to folds1.hdf5\n",
      "Epoch 29/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3425 - val_loss: 0.5397\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.36392\n",
      "Epoch 30/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.4098 - val_loss: 0.4073\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.36392\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3544 - val_loss: 0.4177\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.36392\n",
      "Epoch 32/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.3521 - val_loss: 0.4666\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.36392\n",
      "Epoch 33/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3838 - val_loss: 0.4314\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.36392\n",
      "Epoch 34/200\n",
      "236/236 [==============================] - 198s 841ms/step - loss: 0.3306 - val_loss: 0.3321\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.36392 to 0.33212, saving model to folds1.hdf5\n",
      "Epoch 35/200\n",
      "236/236 [==============================] - 210s 889ms/step - loss: 0.3208 - val_loss: 0.3210\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.33212 to 0.32096, saving model to folds1.hdf5\n",
      "Epoch 36/200\n",
      "236/236 [==============================] - 205s 871ms/step - loss: 0.3058 - val_loss: 0.3277\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.32096\n",
      "Epoch 37/200\n",
      "236/236 [==============================] - 205s 869ms/step - loss: 0.3179 - val_loss: 0.3106\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.32096 to 0.31062, saving model to folds1.hdf5\n",
      "Epoch 38/200\n",
      "236/236 [==============================] - 205s 868ms/step - loss: 0.2693 - val_loss: 0.3616\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31062\n",
      "Epoch 39/200\n",
      "236/236 [==============================] - 204s 863ms/step - loss: 0.2950 - val_loss: 0.3069\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.31062 to 0.30685, saving model to folds1.hdf5\n",
      "Epoch 40/200\n",
      "236/236 [==============================] - 203s 861ms/step - loss: 0.2637 - val_loss: 0.3052\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.30685 to 0.30518, saving model to folds1.hdf5\n",
      "Epoch 41/200\n",
      "236/236 [==============================] - 203s 861ms/step - loss: 0.2629 - val_loss: 0.3265\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.30518\n",
      "Epoch 42/200\n",
      "236/236 [==============================] - 203s 860ms/step - loss: 0.2727 - val_loss: 0.3763\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.30518\n",
      "Epoch 43/200\n",
      "236/236 [==============================] - 204s 865ms/step - loss: 0.2958 - val_loss: 0.3068\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30518\n",
      "Epoch 44/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.2797 - val_loss: 0.3093\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.30518\n",
      "Epoch 45/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.2607 - val_loss: 0.3018\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.30518 to 0.30181, saving model to folds1.hdf5\n",
      "Epoch 46/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.2506 - val_loss: 0.3335\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.30181\n",
      "Epoch 47/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.2830 - val_loss: 0.4866\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.30181\n",
      "Epoch 48/200\n",
      "236/236 [==============================] - 201s 850ms/step - loss: 0.3571 - val_loss: 0.2994\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.30181 to 0.29942, saving model to folds1.hdf5\n",
      "Epoch 49/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.2617 - val_loss: 0.3016\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.29942\n",
      "Epoch 50/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.2570 - val_loss: 0.3012\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.29942\n",
      "Epoch 51/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.2498 - val_loss: 0.2887\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.29942 to 0.28874, saving model to folds1.hdf5\n",
      "Epoch 52/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.2579 - val_loss: 0.4339\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.28874\n",
      "Epoch 53/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.2754 - val_loss: 0.2862\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.28874 to 0.28618, saving model to folds1.hdf5\n",
      "Epoch 54/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.2431 - val_loss: 0.2974\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.28618\n",
      "Epoch 55/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.2426 - val_loss: 0.2838\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.28618 to 0.28379, saving model to folds1.hdf5\n",
      "Epoch 56/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.2420 - val_loss: 0.2949\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.28379\n",
      "Epoch 57/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2323 - val_loss: 0.2683\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.28379 to 0.26828, saving model to folds1.hdf5\n",
      "Epoch 58/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.2293 - val_loss: 0.2924\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.26828\n",
      "Epoch 59/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.2282 - val_loss: 0.2786\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.26828\n",
      "Epoch 60/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.2177 - val_loss: 0.3040\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.26828\n",
      "Epoch 61/200\n",
      "236/236 [==============================] - 197s 834ms/step - loss: 0.2505 - val_loss: 0.3020\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.26828\n",
      "Epoch 62/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.2442 - val_loss: 0.2753\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.26828\n",
      "Epoch 63/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.2195 - val_loss: 0.2562\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.26828 to 0.25622, saving model to folds1.hdf5\n",
      "Epoch 64/200\n",
      "236/236 [==============================] - 192s 814ms/step - loss: 0.2008 - val_loss: 0.2678\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.25622\n",
      "Epoch 65/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.2025 - val_loss: 0.2732\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.25622\n",
      "Epoch 66/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.2031 - val_loss: 0.2588\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.25622\n",
      "Epoch 67/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.2075 - val_loss: 0.2870\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.25622\n",
      "Epoch 68/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2089 - val_loss: 0.2644\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.25622\n",
      "Epoch 69/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.2161 - val_loss: 0.2474\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.25622 to 0.24739, saving model to folds1.hdf5\n",
      "Epoch 70/200\n",
      "236/236 [==============================] - 192s 816ms/step - loss: 0.1921 - val_loss: 0.2986\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24739\n",
      "Epoch 71/200\n",
      "236/236 [==============================] - 194s 821ms/step - loss: 0.2724 - val_loss: 0.2751\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24739\n",
      "Epoch 72/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.2094 - val_loss: 0.2587\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24739\n",
      "Epoch 73/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.2028 - val_loss: 0.2469\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.24739 to 0.24692, saving model to folds1.hdf5\n",
      "Epoch 74/200\n",
      "236/236 [==============================] - 196s 831ms/step - loss: 0.1868 - val_loss: 0.2398\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.24692 to 0.23982, saving model to folds1.hdf5\n",
      "Epoch 75/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.1778 - val_loss: 0.2504\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.23982\n",
      "Epoch 76/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1853 - val_loss: 0.2787\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.23982\n",
      "Epoch 77/200\n",
      "236/236 [==============================] - 194s 822ms/step - loss: 0.2397 - val_loss: 0.2737\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.23982\n",
      "Epoch 78/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.1931 - val_loss: 0.2631\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.23982\n",
      "Epoch 79/200\n",
      "236/236 [==============================] - 196s 831ms/step - loss: 0.1850 - val_loss: 0.2610\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.23982\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 193s 816ms/step - loss: 0.1806 - val_loss: 0.2464\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.23982\n",
      "Epoch 81/200\n",
      "236/236 [==============================] - 194s 823ms/step - loss: 0.1982 - val_loss: 0.2520\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.23982\n",
      "Epoch 82/200\n",
      "236/236 [==============================] - 194s 820ms/step - loss: 0.1759 - val_loss: 0.2403\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.23982\n",
      "Epoch 83/200\n",
      "236/236 [==============================] - 197s 834ms/step - loss: 0.2000 - val_loss: 0.2429\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.23982\n",
      "Epoch 84/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.1828 - val_loss: 0.2489\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.23982\n",
      "Epoch 85/200\n",
      "236/236 [==============================] - 196s 831ms/step - loss: 0.1546 - val_loss: 0.2154\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.23982 to 0.21539, saving model to folds1.hdf5\n",
      "Epoch 86/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1396 - val_loss: 0.2121\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.21539 to 0.21213, saving model to folds1.hdf5\n",
      "Epoch 87/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1323 - val_loss: 0.2054\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.21213 to 0.20544, saving model to folds1.hdf5\n",
      "Epoch 88/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.1286 - val_loss: 0.2042\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.20544 to 0.20424, saving model to folds1.hdf5\n",
      "Epoch 89/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1235 - val_loss: 0.2066\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.20424\n",
      "Epoch 90/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.1264 - val_loss: 0.2083\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.20424\n",
      "Epoch 91/200\n",
      "236/236 [==============================] - 193s 817ms/step - loss: 0.1249 - val_loss: 0.2051\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.20424\n",
      "Epoch 92/200\n",
      "236/236 [==============================] - 195s 826ms/step - loss: 0.1298 - val_loss: 0.2229\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.20424\n",
      "Epoch 93/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.1343 - val_loss: 0.2155\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.20424\n",
      "Epoch 94/200\n",
      "236/236 [==============================] - 196s 829ms/step - loss: 0.1333 - val_loss: 0.2044\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.20424\n",
      "Epoch 95/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.1293 - val_loss: 0.2053\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.20424\n",
      "Epoch 96/200\n",
      "236/236 [==============================] - 194s 820ms/step - loss: 0.1256 - val_loss: 0.2154\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.20424\n",
      "Epoch 97/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.1249 - val_loss: 0.2080\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.20424\n",
      "Epoch 98/200\n",
      "236/236 [==============================] - 195s 827ms/step - loss: 0.1234 - val_loss: 0.2190\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.20424\n",
      "Epoch 99/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.1154 - val_loss: 0.1962\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.20424 to 0.19616, saving model to folds1.hdf5\n",
      "Epoch 100/200\n",
      "236/236 [==============================] - 205s 868ms/step - loss: 0.1040 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.19616\n",
      "Epoch 101/200\n",
      "236/236 [==============================] - 201s 850ms/step - loss: 0.1032 - val_loss: 0.1929\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.19616 to 0.19285, saving model to folds1.hdf5\n",
      "Epoch 102/200\n",
      "236/236 [==============================] - 199s 844ms/step - loss: 0.0995 - val_loss: 0.1935\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.19285\n",
      "Epoch 103/200\n",
      "236/236 [==============================] - 194s 823ms/step - loss: 0.1009 - val_loss: 0.1953\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.19285\n",
      "Epoch 104/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.1003 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.19285 to 0.19260, saving model to folds1.hdf5\n",
      "Epoch 105/200\n",
      "236/236 [==============================] - 193s 820ms/step - loss: 0.0992 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.19260 to 0.19251, saving model to folds1.hdf5\n",
      "Epoch 106/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.0976 - val_loss: 0.1937\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.19251\n",
      "Epoch 107/200\n",
      "236/236 [==============================] - 197s 833ms/step - loss: 0.0971 - val_loss: 0.1910\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.19251 to 0.19096, saving model to folds1.hdf5\n",
      "Epoch 108/200\n",
      "236/236 [==============================] - 199s 842ms/step - loss: 0.0967 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.19096\n",
      "Epoch 109/200\n",
      "236/236 [==============================] - 199s 843ms/step - loss: 0.0956 - val_loss: 0.1906\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.19096 to 0.19057, saving model to folds1.hdf5\n",
      "Epoch 110/200\n",
      "236/236 [==============================] - 198s 838ms/step - loss: 0.0938 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.19057\n",
      "Epoch 111/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.0949 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.19057\n",
      "Epoch 112/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.0969 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.19057 to 0.18991, saving model to folds1.hdf5\n",
      "Epoch 113/200\n",
      "236/236 [==============================] - 194s 824ms/step - loss: 0.0941 - val_loss: 0.1913\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18991\n",
      "Epoch 114/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.0942 - val_loss: 0.1902\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18991\n",
      "Epoch 115/200\n",
      "236/236 [==============================] - 204s 865ms/step - loss: 0.0919 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18991\n",
      "Epoch 116/200\n",
      "236/236 [==============================] - 206s 872ms/step - loss: 0.0908 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18991\n",
      "Epoch 117/200\n",
      "236/236 [==============================] - 205s 868ms/step - loss: 0.0933 - val_loss: 0.2043\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18991\n",
      "Epoch 118/200\n",
      "236/236 [==============================] - 202s 855ms/step - loss: 0.1026 - val_loss: 0.1945\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18991\n",
      "Epoch 119/200\n",
      "236/236 [==============================] - 203s 859ms/step - loss: 0.0932 - val_loss: 0.1922\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.18991\n",
      "Epoch 120/200\n",
      "236/236 [==============================] - 201s 853ms/step - loss: 0.0899 - val_loss: 0.1893\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.18991 to 0.18933, saving model to folds1.hdf5\n",
      "Epoch 121/200\n",
      "236/236 [==============================] - 202s 857ms/step - loss: 0.0886 - val_loss: 0.1895\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.18933\n",
      "Epoch 122/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.0888 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18933\n",
      "Epoch 123/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.0886 - val_loss: 0.1892\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.18933 to 0.18919, saving model to folds1.hdf5\n",
      "Epoch 124/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.0870 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18919\n",
      "Epoch 125/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.0877 - val_loss: 0.1923\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.18919\n",
      "Epoch 126/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.0911 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.18919\n",
      "Epoch 127/200\n",
      "236/236 [==============================] - 199s 842ms/step - loss: 0.0862 - val_loss: 0.1901\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18919\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 199s 841ms/step - loss: 0.0865 - val_loss: 0.1883\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.18919 to 0.18827, saving model to folds1.hdf5\n",
      "Epoch 129/200\n",
      "236/236 [==============================] - 196s 831ms/step - loss: 0.0858 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.18827 to 0.18818, saving model to folds1.hdf5\n",
      "Epoch 130/200\n",
      "236/236 [==============================] - 197s 834ms/step - loss: 0.0849 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.18818\n",
      "Epoch 131/200\n",
      "236/236 [==============================] - 195s 827ms/step - loss: 0.0841 - val_loss: 0.1891\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.18818\n",
      "Epoch 132/200\n",
      "236/236 [==============================] - 195s 826ms/step - loss: 0.0835 - val_loss: 0.1876\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.18818 to 0.18761, saving model to folds1.hdf5\n",
      "Epoch 133/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.0842 - val_loss: 0.1883\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.18761\n",
      "Epoch 134/200\n",
      "236/236 [==============================] - 197s 835ms/step - loss: 0.0845 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.18761 to 0.18719, saving model to folds1.hdf5\n",
      "Epoch 135/200\n",
      "236/236 [==============================] - 202s 856ms/step - loss: 0.0840 - val_loss: 0.1885\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.18719\n",
      "Epoch 136/200\n",
      "236/236 [==============================] - 212s 898ms/step - loss: 0.0859 - val_loss: 0.1878\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.18719\n",
      "Epoch 137/200\n",
      "236/236 [==============================] - 225s 954ms/step - loss: 0.0832 - val_loss: 0.1884\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.18719\n",
      "Epoch 138/200\n",
      "236/236 [==============================] - 231s 978ms/step - loss: 0.0834 - val_loss: 0.1875\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.18719\n",
      "Epoch 139/200\n",
      "236/236 [==============================] - 226s 957ms/step - loss: 0.0808 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.18719\n",
      "Epoch 140/200\n",
      "236/236 [==============================] - 224s 950ms/step - loss: 0.0813 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.18719 to 0.18692, saving model to folds1.hdf5\n",
      "Epoch 141/200\n",
      "236/236 [==============================] - 229s 971ms/step - loss: 0.0812 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.18692 to 0.18565, saving model to folds1.hdf5\n",
      "Epoch 142/200\n",
      "236/236 [==============================] - 225s 952ms/step - loss: 0.0795 - val_loss: 0.1871\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.18565\n",
      "Epoch 143/200\n",
      "236/236 [==============================] - 228s 968ms/step - loss: 0.0806 - val_loss: 0.1881\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.18565\n",
      "Epoch 144/200\n",
      "236/236 [==============================] - 231s 980ms/step - loss: 0.0798 - val_loss: 0.1862\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.18565\n",
      "Epoch 145/200\n",
      "236/236 [==============================] - 227s 963ms/step - loss: 0.0798 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.18565\n",
      "Epoch 146/200\n",
      "236/236 [==============================] - 226s 957ms/step - loss: 0.0785 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.18565\n",
      "Epoch 147/200\n",
      "236/236 [==============================] - 225s 953ms/step - loss: 0.0789 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.18565\n",
      "Epoch 148/200\n",
      "236/236 [==============================] - 226s 959ms/step - loss: 0.0781 - val_loss: 0.1887\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.18565\n",
      "Epoch 149/200\n",
      "236/236 [==============================] - 223s 944ms/step - loss: 0.0788 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.18565\n",
      "Epoch 150/200\n",
      "236/236 [==============================] - 206s 871ms/step - loss: 0.0784 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.18565 to 0.18561, saving model to folds1.hdf5\n",
      "Epoch 151/200\n",
      "236/236 [==============================] - 208s 883ms/step - loss: 0.0771 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.18561\n",
      "Epoch 152/200\n",
      "236/236 [==============================] - 208s 880ms/step - loss: 0.0722 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.18561 to 0.18339, saving model to folds1.hdf5\n",
      "Epoch 153/200\n",
      "236/236 [==============================] - 206s 875ms/step - loss: 0.0689 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.18339 to 0.18325, saving model to folds1.hdf5\n",
      "Epoch 154/200\n",
      "236/236 [==============================] - 207s 877ms/step - loss: 0.0682 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.18325 to 0.18313, saving model to folds1.hdf5\n",
      "Epoch 155/200\n",
      "236/236 [==============================] - 208s 881ms/step - loss: 0.0680 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.18313\n",
      "Epoch 156/200\n",
      "236/236 [==============================] - 209s 886ms/step - loss: 0.0681 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.18313\n",
      "Epoch 157/200\n",
      "236/236 [==============================] - 209s 885ms/step - loss: 0.0682 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.18313\n",
      "Epoch 158/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.0677 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.18313\n",
      "Epoch 159/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.0679 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.18313\n",
      "Epoch 160/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.0674 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.18313\n",
      "Epoch 161/200\n",
      "236/236 [==============================] - 196s 830ms/step - loss: 0.0676 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.18313\n",
      "Epoch 162/200\n",
      "236/236 [==============================] - 198s 839ms/step - loss: 0.0666 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.18313\n",
      "Epoch 163/200\n",
      "236/236 [==============================] - 192s 816ms/step - loss: 0.0666 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.18313\n",
      "Epoch 164/200\n",
      "236/236 [==============================] - 194s 821ms/step - loss: 0.0669 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.18313\n",
      "Epoch 165/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.0641 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.18313\n",
      "Epoch 166/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.0627 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.18313 to 0.18286, saving model to folds1.hdf5\n",
      "Epoch 167/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.0627 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.18286 to 0.18285, saving model to folds1.hdf5\n",
      "Epoch 168/200\n",
      "236/236 [==============================] - 198s 840ms/step - loss: 0.0629 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.18285\n",
      "Epoch 169/200\n",
      "236/236 [==============================] - 199s 843ms/step - loss: 0.0618 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.18285 to 0.18266, saving model to folds1.hdf5\n",
      "Epoch 170/200\n",
      "236/236 [==============================] - 194s 823ms/step - loss: 0.0619 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.18266\n",
      "Epoch 171/200\n",
      "236/236 [==============================] - 194s 824ms/step - loss: 0.0615 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.18266\n",
      "Epoch 172/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.0618 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.18266\n",
      "Epoch 173/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0614 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.18266\n",
      "Epoch 174/200\n",
      "236/236 [==============================] - 192s 814ms/step - loss: 0.0614 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.18266\n",
      "Epoch 175/200\n",
      "236/236 [==============================] - 193s 817ms/step - loss: 0.0609 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.18266\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 191s 810ms/step - loss: 0.0615 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.18266\n",
      "Epoch 177/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.0613 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.18266\n",
      "Epoch 178/200\n",
      "236/236 [==============================] - 198s 838ms/step - loss: 0.0601 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.18266\n",
      "Epoch 179/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.0607 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.18266\n",
      "Epoch 180/200\n",
      "236/236 [==============================] - 198s 837ms/step - loss: 0.0589 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.18266\n",
      "Epoch 181/200\n",
      "236/236 [==============================] - 198s 839ms/step - loss: 0.0587 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.18266\n",
      "Epoch 182/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.0585 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.18266 to 0.18260, saving model to folds1.hdf5\n",
      "Epoch 183/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.0582 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.18260\n",
      "Epoch 184/200\n",
      "236/236 [==============================] - 209s 886ms/step - loss: 0.0584 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.18260 to 0.18255, saving model to folds1.hdf5\n",
      "Epoch 185/200\n",
      "236/236 [==============================] - 199s 842ms/step - loss: 0.0582 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.18255\n",
      "Epoch 186/200\n",
      "236/236 [==============================] - 198s 838ms/step - loss: 0.0586 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.18255\n",
      "Epoch 187/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.0579 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18255\n",
      "Epoch 188/200\n",
      "236/236 [==============================] - 207s 875ms/step - loss: 0.0577 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.18255\n",
      "Epoch 189/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.0587 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.18255\n",
      "Epoch 190/200\n",
      "236/236 [==============================] - 194s 824ms/step - loss: 0.0578 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.18255\n",
      "Epoch 191/200\n",
      "236/236 [==============================] - 201s 853ms/step - loss: 0.0579 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.18255\n",
      "Epoch 192/200\n",
      "236/236 [==============================] - 198s 838ms/step - loss: 0.0577 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.18255\n",
      "Epoch 193/200\n",
      "236/236 [==============================] - 204s 864ms/step - loss: 0.0575 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.18255\n",
      "Epoch 194/200\n",
      "236/236 [==============================] - 193s 820ms/step - loss: 0.0577 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.18255\n",
      "Epoch 195/200\n",
      "236/236 [==============================] - 194s 821ms/step - loss: 0.0571 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.18255\n",
      "Epoch 196/200\n",
      "236/236 [==============================] - 193s 817ms/step - loss: 0.0565 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.18255\n",
      "Epoch 197/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.0562 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.18255\n",
      "Epoch 198/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.0571 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.18255\n",
      "Epoch 199/200\n",
      "236/236 [==============================] - 198s 841ms/step - loss: 0.0573 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.18255\n",
      "Epoch 200/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.0564 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.18255\n",
      "197/197 - 34s\n",
      "--------------- > Fold 2 < ---------------\n",
      "Epoch 1/200\n",
      "236/236 [==============================] - 210s 851ms/step - loss: 4.1333 - val_loss: 1.0247\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02471, saving model to folds2.hdf5\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 194s 822ms/step - loss: 0.9631 - val_loss: 0.7522\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02471 to 0.75216, saving model to folds2.hdf5\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.7866 - val_loss: 0.7079\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75216 to 0.70785, saving model to folds2.hdf5\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.7027 - val_loss: 0.6315\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.70785 to 0.63150, saving model to folds2.hdf5\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 199s 845ms/step - loss: 0.6413 - val_loss: 0.6024\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63150 to 0.60245, saving model to folds2.hdf5\n",
      "Epoch 6/200\n",
      "236/236 [==============================] - 201s 853ms/step - loss: 0.6083 - val_loss: 0.5605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.60245 to 0.56046, saving model to folds2.hdf5\n",
      "Epoch 7/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.5496 - val_loss: 0.5227\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56046 to 0.52266, saving model to folds2.hdf5\n",
      "Epoch 8/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.5366 - val_loss: 0.6527\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52266\n",
      "Epoch 9/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.5430 - val_loss: 0.4853\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.52266 to 0.48528, saving model to folds2.hdf5\n",
      "Epoch 10/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.5134 - val_loss: 0.4946\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48528\n",
      "Epoch 11/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.4719 - val_loss: 0.4356\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48528 to 0.43557, saving model to folds2.hdf5\n",
      "Epoch 12/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.4627 - val_loss: 0.6234\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43557\n",
      "Epoch 13/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.5488 - val_loss: 0.4681\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43557\n",
      "Epoch 14/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.4552 - val_loss: 0.4211\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.43557 to 0.42106, saving model to folds2.hdf5\n",
      "Epoch 15/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.4571 - val_loss: 0.4330\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42106\n",
      "Epoch 16/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.4160 - val_loss: 0.3995\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.42106 to 0.39946, saving model to folds2.hdf5\n",
      "Epoch 17/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.4231 - val_loss: 0.6205\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39946\n",
      "Epoch 18/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.4397 - val_loss: 0.4201\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39946\n",
      "Epoch 19/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.4242 - val_loss: 0.4296\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.39946\n",
      "Epoch 20/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.4144 - val_loss: 0.4204\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.39946\n",
      "Epoch 21/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.3985 - val_loss: 0.4236\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.39946\n",
      "Epoch 22/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.4116 - val_loss: 0.4526\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.39946\n",
      "Epoch 23/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.4080 - val_loss: 0.3885\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.39946 to 0.38850, saving model to folds2.hdf5\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 189s 801ms/step - loss: 0.3483 - val_loss: 0.3677\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.38850 to 0.36774, saving model to folds2.hdf5\n",
      "Epoch 25/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.3417 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.36774\n",
      "Epoch 26/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.3307 - val_loss: 0.3596\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.36774 to 0.35960, saving model to folds2.hdf5\n",
      "Epoch 27/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.3510 - val_loss: 0.3503\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.35960 to 0.35026, saving model to folds2.hdf5\n",
      "Epoch 28/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.3284 - val_loss: 0.4056\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.35026\n",
      "Epoch 29/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.3585 - val_loss: 0.3333\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.35026 to 0.33327, saving model to folds2.hdf5\n",
      "Epoch 30/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.3182 - val_loss: 0.3400\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.33327\n",
      "Epoch 31/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.3127 - val_loss: 0.3400\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.33327\n",
      "Epoch 32/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.3094 - val_loss: 0.3669\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.33327\n",
      "Epoch 33/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.3037 - val_loss: 0.2999\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.33327 to 0.29986, saving model to folds2.hdf5\n",
      "Epoch 34/200\n",
      "236/236 [==============================] - 202s 854ms/step - loss: 0.2755 - val_loss: 0.3286\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.29986\n",
      "Epoch 35/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.2972 - val_loss: 0.3261\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.29986\n",
      "Epoch 36/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.2902 - val_loss: 0.3132\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.29986\n",
      "Epoch 37/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.2859 - val_loss: 0.3202\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.29986\n",
      "Epoch 38/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.3040 - val_loss: 0.2953\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.29986 to 0.29531, saving model to folds2.hdf5\n",
      "Epoch 39/200\n",
      "236/236 [==============================] - 196s 831ms/step - loss: 0.2717 - val_loss: 0.3108\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.29531\n",
      "Epoch 40/200\n",
      "236/236 [==============================] - 199s 846ms/step - loss: 0.2809 - val_loss: 0.3529\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29531\n",
      "Epoch 41/200\n",
      "236/236 [==============================] - 200s 846ms/step - loss: 0.2996 - val_loss: 0.3716\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29531\n",
      "Epoch 42/200\n",
      "236/236 [==============================] - 197s 834ms/step - loss: 0.2875 - val_loss: 0.2845\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.29531 to 0.28453, saving model to folds2.hdf5\n",
      "Epoch 43/200\n",
      "236/236 [==============================] - 197s 835ms/step - loss: 0.2524 - val_loss: 0.2680\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.28453 to 0.26803, saving model to folds2.hdf5\n",
      "Epoch 44/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.2572 - val_loss: 0.2673\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.26803 to 0.26733, saving model to folds2.hdf5\n",
      "Epoch 45/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.2560 - val_loss: 0.2599\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.26733 to 0.25985, saving model to folds2.hdf5\n",
      "Epoch 46/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2325 - val_loss: 0.2755\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.25985\n",
      "Epoch 47/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.2599 - val_loss: 0.2646\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.25985\n",
      "Epoch 48/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.2353 - val_loss: 0.3105\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.25985\n",
      "Epoch 49/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.2538 - val_loss: 0.2855\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.25985\n",
      "Epoch 50/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.2506 - val_loss: 0.2695\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.25985\n",
      "Epoch 51/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.2413 - val_loss: 0.2834\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.25985\n",
      "Epoch 52/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.2315 - val_loss: 0.2875\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.25985\n",
      "Epoch 53/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.2460 - val_loss: 0.3338\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.25985\n",
      "Epoch 54/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.2572 - val_loss: 0.2996\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.25985\n",
      "Epoch 55/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.2730 - val_loss: 0.3155\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.25985\n",
      "Epoch 56/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.2110 - val_loss: 0.2257\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.25985 to 0.22572, saving model to folds2.hdf5\n",
      "Epoch 57/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1726 - val_loss: 0.2175\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.22572 to 0.21748, saving model to folds2.hdf5\n",
      "Epoch 58/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.1642 - val_loss: 0.2199\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21748\n",
      "Epoch 59/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.1638 - val_loss: 0.2194\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21748\n",
      "Epoch 60/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.1636 - val_loss: 0.2171\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.21748 to 0.21710, saving model to folds2.hdf5\n",
      "Epoch 61/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.1619 - val_loss: 0.2150\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.21710 to 0.21500, saving model to folds2.hdf5\n",
      "Epoch 62/200\n",
      "236/236 [==============================] - 200s 850ms/step - loss: 0.1583 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.21500\n",
      "Epoch 63/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1617 - val_loss: 0.2145\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.21500 to 0.21446, saving model to folds2.hdf5\n",
      "Epoch 64/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.1609 - val_loss: 0.2165\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.21446\n",
      "Epoch 65/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.1563 - val_loss: 0.2142\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.21446 to 0.21419, saving model to folds2.hdf5\n",
      "Epoch 66/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.1754 - val_loss: 0.2130\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.21419 to 0.21305, saving model to folds2.hdf5\n",
      "Epoch 67/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.1539 - val_loss: 0.2094\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.21305 to 0.20938, saving model to folds2.hdf5\n",
      "Epoch 68/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.1528 - val_loss: 0.2113\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.20938\n",
      "Epoch 69/200\n",
      "236/236 [==============================] - 203s 862ms/step - loss: 0.1566 - val_loss: 0.2078\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.20938 to 0.20778, saving model to folds2.hdf5\n",
      "Epoch 70/200\n",
      "236/236 [==============================] - 199s 843ms/step - loss: 0.1648 - val_loss: 0.2022\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.20778 to 0.20222, saving model to folds2.hdf5\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 203s 858ms/step - loss: 0.1423 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.20222 to 0.20072, saving model to folds2.hdf5\n",
      "Epoch 72/200\n",
      "236/236 [==============================] - 202s 857ms/step - loss: 0.1454 - val_loss: 0.2152\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.20072\n",
      "Epoch 73/200\n",
      "236/236 [==============================] - 200s 850ms/step - loss: 0.1405 - val_loss: 0.2101\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.20072\n",
      "Epoch 74/200\n",
      "236/236 [==============================] - 199s 843ms/step - loss: 0.1383 - val_loss: 0.1998\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.20072 to 0.19976, saving model to folds2.hdf5\n",
      "Epoch 75/200\n",
      "236/236 [==============================] - 195s 825ms/step - loss: 0.1378 - val_loss: 0.2057\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19976\n",
      "Epoch 76/200\n",
      "236/236 [==============================] - 199s 845ms/step - loss: 0.1404 - val_loss: 0.2044\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19976\n",
      "Epoch 77/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.1379 - val_loss: 0.1980\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.19976 to 0.19801, saving model to folds2.hdf5\n",
      "Epoch 78/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.3336 - val_loss: 0.2891\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19801\n",
      "Epoch 79/200\n",
      "236/236 [==============================] - 199s 841ms/step - loss: 0.2198 - val_loss: 0.2407\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19801\n",
      "Epoch 80/200\n",
      "236/236 [==============================] - 201s 854ms/step - loss: 0.1815 - val_loss: 0.2237\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19801\n",
      "Epoch 81/200\n",
      "236/236 [==============================] - 204s 865ms/step - loss: 0.1744 - val_loss: 0.2079\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19801\n",
      "Epoch 82/200\n",
      "236/236 [==============================] - 201s 853ms/step - loss: 0.1471 - val_loss: 0.2117\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19801\n",
      "Epoch 83/200\n",
      "236/236 [==============================] - 203s 858ms/step - loss: 0.1441 - val_loss: 0.2066\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19801\n",
      "Epoch 84/200\n",
      "236/236 [==============================] - 194s 820ms/step - loss: 0.1348 - val_loss: 0.2051\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19801\n",
      "Epoch 85/200\n",
      "236/236 [==============================] - 194s 823ms/step - loss: 0.1299 - val_loss: 0.2088\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.19801\n",
      "Epoch 86/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.1281 - val_loss: 0.2227\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19801\n",
      "Epoch 87/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.1433 - val_loss: 0.2032\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.19801\n",
      "Epoch 88/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.1232 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.19801 to 0.18738, saving model to folds2.hdf5\n",
      "Epoch 89/200\n",
      "236/236 [==============================] - 192s 814ms/step - loss: 0.1112 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.18738 to 0.18722, saving model to folds2.hdf5\n",
      "Epoch 90/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.1086 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.18722 to 0.18688, saving model to folds2.hdf5\n",
      "Epoch 91/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.1080 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.18688 to 0.18546, saving model to folds2.hdf5\n",
      "Epoch 92/200\n",
      "236/236 [==============================] - 192s 812ms/step - loss: 0.1076 - val_loss: 0.1930\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18546\n",
      "Epoch 93/200\n",
      "236/236 [==============================] - 196s 830ms/step - loss: 0.1149 - val_loss: 0.1907\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.18546\n",
      "Epoch 94/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.1109 - val_loss: 0.1961\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18546\n",
      "Epoch 95/200\n",
      "236/236 [==============================] - 199s 842ms/step - loss: 0.1083 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.18546 to 0.18362, saving model to folds2.hdf5\n",
      "Epoch 96/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.1039 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18362\n",
      "Epoch 97/200\n",
      "236/236 [==============================] - 191s 811ms/step - loss: 0.1033 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18362\n",
      "Epoch 98/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.1023 - val_loss: 0.1841\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18362\n",
      "Epoch 99/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.1012 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18362\n",
      "Epoch 100/200\n",
      "236/236 [==============================] - 197s 836ms/step - loss: 0.1006 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18362\n",
      "Epoch 101/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.1007 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.18362 to 0.18256, saving model to folds2.hdf5\n",
      "Epoch 102/200\n",
      "236/236 [==============================] - 195s 826ms/step - loss: 0.0994 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18256\n",
      "Epoch 103/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.0993 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18256\n",
      "Epoch 104/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.0992 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18256\n",
      "Epoch 105/200\n",
      "236/236 [==============================] - 191s 810ms/step - loss: 0.0967 - val_loss: 0.1825\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.18256 to 0.18248, saving model to folds2.hdf5\n",
      "Epoch 106/200\n",
      "236/236 [==============================] - 194s 822ms/step - loss: 0.0976 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18248\n",
      "Epoch 107/200\n",
      "236/236 [==============================] - 199s 844ms/step - loss: 0.0970 - val_loss: 0.1845\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18248\n",
      "Epoch 108/200\n",
      "236/236 [==============================] - 197s 833ms/step - loss: 0.0974 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.18248 to 0.18243, saving model to folds2.hdf5\n",
      "Epoch 109/200\n",
      "236/236 [==============================] - 194s 824ms/step - loss: 0.0973 - val_loss: 0.1913\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18243\n",
      "Epoch 110/200\n",
      "236/236 [==============================] - 193s 818ms/step - loss: 0.1018 - val_loss: 0.1880\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18243\n",
      "Epoch 111/200\n",
      "236/236 [==============================] - 200s 849ms/step - loss: 0.1215 - val_loss: 0.1896\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18243\n",
      "Epoch 112/200\n",
      "236/236 [==============================] - 198s 840ms/step - loss: 0.1067 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.18243 to 0.18060, saving model to folds2.hdf5\n",
      "Epoch 113/200\n",
      "236/236 [==============================] - 197s 833ms/step - loss: 0.0965 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.18060 to 0.18023, saving model to folds2.hdf5\n",
      "Epoch 114/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.0923 - val_loss: 0.2127\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18023\n",
      "Epoch 115/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.1064 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18023\n",
      "Epoch 116/200\n",
      "236/236 [==============================] - 197s 835ms/step - loss: 0.0939 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18023\n",
      "Epoch 117/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.0921 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.18023 to 0.17993, saving model to folds2.hdf5\n",
      "Epoch 118/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.0916 - val_loss: 0.1850\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17993\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 192s 813ms/step - loss: 0.0968 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17993\n",
      "Epoch 120/200\n",
      "236/236 [==============================] - 191s 812ms/step - loss: 0.0916 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.17993 to 0.17717, saving model to folds2.hdf5\n",
      "Epoch 121/200\n",
      "236/236 [==============================] - 192s 813ms/step - loss: 0.0900 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17717\n",
      "Epoch 122/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0908 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17717\n",
      "Epoch 123/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0901 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17717\n",
      "Epoch 124/200\n",
      "236/236 [==============================] - 191s 808ms/step - loss: 0.0895 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17717\n",
      "Epoch 125/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0892 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17717\n",
      "Epoch 126/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0882 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17717\n",
      "Epoch 127/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0885 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17717\n",
      "Epoch 128/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0877 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17717\n",
      "Epoch 129/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0889 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17717\n",
      "Epoch 130/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0855 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17717\n",
      "Epoch 131/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0798 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.17717 to 0.17383, saving model to folds2.hdf5\n",
      "Epoch 132/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.0762 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.17383 to 0.17379, saving model to folds2.hdf5\n",
      "Epoch 133/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0758 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17379\n",
      "Epoch 134/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0765 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17379\n",
      "Epoch 135/200\n",
      "236/236 [==============================] - 190s 807ms/step - loss: 0.0749 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17379\n",
      "Epoch 136/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.0751 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17379\n",
      "Epoch 137/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.0746 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17379\n",
      "Epoch 138/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0737 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17379\n",
      "Epoch 139/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0743 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17379\n",
      "Epoch 140/200\n",
      "236/236 [==============================] - 191s 809ms/step - loss: 0.0750 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17379\n",
      "Epoch 141/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0738 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17379\n",
      "Epoch 142/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0700 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.17379 to 0.17265, saving model to folds2.hdf5\n",
      "Epoch 143/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0693 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17265\n",
      "Epoch 144/200\n",
      "236/236 [==============================] - 190s 806ms/step - loss: 0.0688 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17265\n",
      "Epoch 145/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.0690 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.17265 to 0.17240, saving model to folds2.hdf5\n",
      "Epoch 146/200\n",
      "236/236 [==============================] - 189s 803ms/step - loss: 0.0685 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17240\n",
      "Epoch 147/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0678 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.17240 to 0.17205, saving model to folds2.hdf5\n",
      "Epoch 148/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.0674 - val_loss: 0.1732\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17205\n",
      "Epoch 149/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0680 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17205\n",
      "Epoch 150/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.0676 - val_loss: 0.1730\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17205\n",
      "Epoch 151/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0672 - val_loss: 0.1725\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17205\n",
      "Epoch 152/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0669 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17205\n",
      "Epoch 153/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0676 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17205\n",
      "Epoch 154/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0668 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17205\n",
      "Epoch 155/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0673 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17205\n",
      "Epoch 156/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0672 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17205\n",
      "Epoch 157/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0664 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17205\n",
      "Epoch 158/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0649 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17205\n",
      "Epoch 159/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.0640 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17205\n",
      "Epoch 160/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0636 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17205\n",
      "Epoch 161/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0638 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17205\n",
      "Epoch 162/200\n",
      "236/236 [==============================] - 189s 801ms/step - loss: 0.0639 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17205\n",
      "Epoch 163/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0633 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17205\n",
      "Epoch 164/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.0636 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17205\n",
      "Epoch 165/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0628 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17205\n",
      "Epoch 166/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0634 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17205\n",
      "Epoch 167/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0629 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17205\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0626 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17205\n",
      "Epoch 169/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0614 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17205\n",
      "Epoch 170/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0614 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.17205 to 0.17200, saving model to folds2.hdf5\n",
      "Epoch 171/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0618 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17200\n",
      "Epoch 172/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0620 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17200\n",
      "Epoch 173/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0624 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17200\n",
      "Epoch 174/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0620 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17200\n",
      "Epoch 175/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0615 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17200\n",
      "Epoch 176/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0620 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17200\n",
      "Epoch 177/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0610 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17200\n",
      "Epoch 178/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0616 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17200\n",
      "Epoch 179/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0610 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17200\n",
      "Epoch 180/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0609 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17200\n",
      "Epoch 181/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0613 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17200\n",
      "Epoch 182/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0610 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17200\n",
      "Epoch 183/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0611 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17200\n",
      "Epoch 184/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0607 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17200\n",
      "Epoch 185/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0613 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17200\n",
      "Epoch 186/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0605 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17200\n",
      "Epoch 187/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0604 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17200\n",
      "Epoch 188/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0612 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17200\n",
      "Epoch 189/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0607 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17200\n",
      "Epoch 190/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0603 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17200\n",
      "Epoch 191/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0601 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17200\n",
      "Epoch 192/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0611 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17200\n",
      "Epoch 193/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0609 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17200\n",
      "Epoch 194/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0602 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17200\n",
      "Epoch 195/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0604 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17200\n",
      "Epoch 196/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0606 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17200\n",
      "Epoch 197/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0603 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17200\n",
      "Epoch 198/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0604 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17200\n",
      "Epoch 199/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0606 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17200\n",
      "Epoch 200/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0598 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17200\n",
      "197/197 - 31s\n",
      "--------------- > Fold 3 < ---------------\n",
      "Epoch 1/200\n",
      "236/236 [==============================] - 196s 796ms/step - loss: 4.0758 - val_loss: 1.0491\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04911, saving model to folds3.hdf5\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.9884 - val_loss: 0.7880\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04911 to 0.78802, saving model to folds3.hdf5\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.7636 - val_loss: 0.6849\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.78802 to 0.68493, saving model to folds3.hdf5\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.6980 - val_loss: 0.7149\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68493\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.6424 - val_loss: 0.6292\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68493 to 0.62921, saving model to folds3.hdf5\n",
      "Epoch 6/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.6011 - val_loss: 0.6120\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62921 to 0.61204, saving model to folds3.hdf5\n",
      "Epoch 7/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.5407 - val_loss: 0.7008\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61204\n",
      "Epoch 8/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.6028 - val_loss: 0.4796\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61204 to 0.47957, saving model to folds3.hdf5\n",
      "Epoch 9/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5186 - val_loss: 0.5638\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.47957\n",
      "Epoch 10/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.5010 - val_loss: 0.5482\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.47957\n",
      "Epoch 11/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.5175 - val_loss: 0.4821\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.47957\n",
      "Epoch 12/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4623 - val_loss: 0.5023\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.47957\n",
      "Epoch 13/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.4708 - val_loss: 0.4768\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47957 to 0.47682, saving model to folds3.hdf5\n",
      "Epoch 14/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4380 - val_loss: 0.4740\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47682 to 0.47403, saving model to folds3.hdf5\n",
      "Epoch 15/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.4453 - val_loss: 0.4978\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47403\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 187s 794ms/step - loss: 0.4761 - val_loss: 0.4365\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.47403 to 0.43648, saving model to folds3.hdf5\n",
      "Epoch 17/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.4460 - val_loss: 0.4209\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.43648 to 0.42092, saving model to folds3.hdf5\n",
      "Epoch 18/200\n",
      "236/236 [==============================] - 188s 794ms/step - loss: 0.4227 - val_loss: 0.4222\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42092\n",
      "Epoch 19/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3852 - val_loss: 0.4090\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.42092 to 0.40904, saving model to folds3.hdf5\n",
      "Epoch 20/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4271 - val_loss: 0.4734\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.40904\n",
      "Epoch 21/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4167 - val_loss: 0.3932\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.40904 to 0.39324, saving model to folds3.hdf5\n",
      "Epoch 22/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4145 - val_loss: 0.5020\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.39324\n",
      "Epoch 23/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.4119 - val_loss: 0.4348\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.39324\n",
      "Epoch 24/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.4073 - val_loss: 0.4197\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.39324\n",
      "Epoch 25/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.3755 - val_loss: 0.4846\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.39324\n",
      "Epoch 26/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4062 - val_loss: 0.5377\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.39324\n",
      "Epoch 27/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4279 - val_loss: 0.7848\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.39324\n",
      "Epoch 28/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.5688 - val_loss: 0.3975\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.39324\n",
      "Epoch 29/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.3610 - val_loss: 0.3957\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.39324\n",
      "Epoch 30/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4528 - val_loss: 0.4123\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.39324\n",
      "Epoch 31/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3949 - val_loss: 0.3745\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.39324 to 0.37451, saving model to folds3.hdf5\n",
      "Epoch 32/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.3950 - val_loss: 0.3993\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.37451\n",
      "Epoch 33/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.3936 - val_loss: 0.3403\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.37451 to 0.34025, saving model to folds3.hdf5\n",
      "Epoch 34/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3240 - val_loss: 0.3596\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.34025\n",
      "Epoch 35/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3544 - val_loss: 0.3591\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.34025\n",
      "Epoch 36/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3299 - val_loss: 0.3123\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.34025 to 0.31235, saving model to folds3.hdf5\n",
      "Epoch 37/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2924 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.31235\n",
      "Epoch 38/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.3405 - val_loss: 0.4010\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31235\n",
      "Epoch 39/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3470 - val_loss: 0.3665\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.31235\n",
      "Epoch 40/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3136 - val_loss: 0.3120\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.31235 to 0.31200, saving model to folds3.hdf5\n",
      "Epoch 41/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2786 - val_loss: 0.3432\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.31200\n",
      "Epoch 42/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3146 - val_loss: 0.3056\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.31200 to 0.30562, saving model to folds3.hdf5\n",
      "Epoch 43/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2795 - val_loss: 0.3107\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.30562\n",
      "Epoch 44/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2796 - val_loss: 0.4377\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.30562\n",
      "Epoch 45/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.3554 - val_loss: 0.3852\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.30562\n",
      "Epoch 46/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2911 - val_loss: 0.2874\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.30562 to 0.28736, saving model to folds3.hdf5\n",
      "Epoch 47/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2876 - val_loss: 0.2896\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.28736\n",
      "Epoch 48/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.2526 - val_loss: 0.2839\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.28736 to 0.28386, saving model to folds3.hdf5\n",
      "Epoch 49/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2530 - val_loss: 0.3198\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.28386\n",
      "Epoch 50/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.3274 - val_loss: 0.3126\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.28386\n",
      "Epoch 51/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2522 - val_loss: 0.3390\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.28386\n",
      "Epoch 52/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.2692 - val_loss: 0.3211\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.28386\n",
      "Epoch 53/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2765 - val_loss: 0.2904\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.28386\n",
      "Epoch 54/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2799 - val_loss: 0.2942\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.28386\n",
      "Epoch 55/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2493 - val_loss: 0.3120\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.28386\n",
      "Epoch 56/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2558 - val_loss: 0.3237\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.28386\n",
      "Epoch 57/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2626 - val_loss: 0.2733\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.28386 to 0.27325, saving model to folds3.hdf5\n",
      "Epoch 58/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2272 - val_loss: 0.2562\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.27325 to 0.25624, saving model to folds3.hdf5\n",
      "Epoch 59/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2327 - val_loss: 0.2867\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.25624\n",
      "Epoch 60/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2357 - val_loss: 0.4434\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.25624\n",
      "Epoch 61/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.3407 - val_loss: 0.3250\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.25624\n",
      "Epoch 62/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2542 - val_loss: 0.2797\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.25624\n",
      "Epoch 63/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2533 - val_loss: 0.3120\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.25624\n",
      "Epoch 64/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2275 - val_loss: 0.2588\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.25624\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2524 - val_loss: 0.3067\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.25624\n",
      "Epoch 66/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2404 - val_loss: 0.3313\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.25624\n",
      "Epoch 67/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2592 - val_loss: 0.2581\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.25624\n",
      "Epoch 68/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.2287 - val_loss: 0.2559\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.25624 to 0.25587, saving model to folds3.hdf5\n",
      "Epoch 69/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2217 - val_loss: 0.2711\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.25587\n",
      "Epoch 70/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2657 - val_loss: 0.2584\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.25587\n",
      "Epoch 71/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2086 - val_loss: 0.3204\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.25587\n",
      "Epoch 72/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2290 - val_loss: 0.2497\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.25587 to 0.24972, saving model to folds3.hdf5\n",
      "Epoch 73/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2336 - val_loss: 0.2727\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24972\n",
      "Epoch 74/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2090 - val_loss: 0.2520\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24972\n",
      "Epoch 75/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2001 - val_loss: 0.3146\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24972\n",
      "Epoch 76/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2406 - val_loss: 0.2702\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24972\n",
      "Epoch 77/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.2140 - val_loss: 0.2474\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.24972 to 0.24740, saving model to folds3.hdf5\n",
      "Epoch 78/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.2186 - val_loss: 0.2762\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24740\n",
      "Epoch 79/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.2021 - val_loss: 0.2588\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24740\n",
      "Epoch 80/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1824 - val_loss: 0.2263\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.24740 to 0.22634, saving model to folds3.hdf5\n",
      "Epoch 81/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.1778 - val_loss: 0.2461\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.22634\n",
      "Epoch 82/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.2178 - val_loss: 0.2287\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.22634\n",
      "Epoch 83/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1741 - val_loss: 0.2500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.22634\n",
      "Epoch 84/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.2058 - val_loss: 0.2310\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.22634\n",
      "Epoch 85/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1869 - val_loss: 0.2278\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.22634\n",
      "Epoch 86/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.1739 - val_loss: 0.2813\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.22634\n",
      "Epoch 87/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2055 - val_loss: 0.2449\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.22634\n",
      "Epoch 88/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2380 - val_loss: 0.2843\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.22634\n",
      "Epoch 89/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.2256 - val_loss: 0.2297\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.22634\n",
      "Epoch 90/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1849 - val_loss: 0.2266\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.22634\n",
      "Epoch 91/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.1841 - val_loss: 0.2605\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.22634\n",
      "Epoch 92/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1830 - val_loss: 0.2150\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.22634 to 0.21496, saving model to folds3.hdf5\n",
      "Epoch 93/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1505 - val_loss: 0.2264\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.21496\n",
      "Epoch 94/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1473 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.21496 to 0.20772, saving model to folds3.hdf5\n",
      "Epoch 95/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1374 - val_loss: 0.2046\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.20772 to 0.20460, saving model to folds3.hdf5\n",
      "Epoch 96/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1331 - val_loss: 0.2015\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.20460 to 0.20153, saving model to folds3.hdf5\n",
      "Epoch 97/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1329 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.20153\n",
      "Epoch 98/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1399 - val_loss: 0.2004\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.20153 to 0.20041, saving model to folds3.hdf5\n",
      "Epoch 99/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1365 - val_loss: 0.2075\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.20041\n",
      "Epoch 100/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1335 - val_loss: 0.2028\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.20041\n",
      "Epoch 101/200\n",
      "236/236 [==============================] - 187s 791ms/step - loss: 0.1271 - val_loss: 0.2068\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.20041\n",
      "Epoch 102/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1275 - val_loss: 0.1963\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.20041 to 0.19627, saving model to folds3.hdf5\n",
      "Epoch 103/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1218 - val_loss: 0.1923\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.19627 to 0.19234, saving model to folds3.hdf5\n",
      "Epoch 104/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.1203 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.19234\n",
      "Epoch 105/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.1220 - val_loss: 0.2122\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.19234\n",
      "Epoch 106/200\n",
      "236/236 [==============================] - 192s 815ms/step - loss: 0.1372 - val_loss: 0.1919\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.19234 to 0.19191, saving model to folds3.hdf5\n",
      "Epoch 107/200\n",
      "236/236 [==============================] - 193s 817ms/step - loss: 0.1426 - val_loss: 0.2087\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.19191\n",
      "Epoch 108/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.1310 - val_loss: 0.1961\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.19191\n",
      "Epoch 109/200\n",
      "236/236 [==============================] - 192s 816ms/step - loss: 0.1205 - val_loss: 0.3662\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.19191\n",
      "Epoch 110/200\n",
      "236/236 [==============================] - 200s 847ms/step - loss: 0.2096 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.19191\n",
      "Epoch 111/200\n",
      "236/236 [==============================] - 331s 1s/step - loss: 0.1390 - val_loss: 0.1946\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.19191\n",
      "Epoch 112/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.1246 - val_loss: 0.2158\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.19191\n",
      "Epoch 113/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.1507 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.19191\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 188s 797ms/step - loss: 0.1251 - val_loss: 0.1938\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.19191\n",
      "Epoch 115/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.1180 - val_loss: 0.1920\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.19191\n",
      "Epoch 116/200\n",
      "236/236 [==============================] - 197s 837ms/step - loss: 0.1338 - val_loss: 0.1890\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.19191 to 0.18897, saving model to folds3.hdf5\n",
      "Epoch 117/200\n",
      "236/236 [==============================] - 206s 873ms/step - loss: 0.1117 - val_loss: 0.1975\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18897\n",
      "Epoch 118/200\n",
      "236/236 [==============================] - 269s 1s/step - loss: 0.1649 - val_loss: 0.1969\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18897\n",
      "Epoch 119/200\n",
      "236/236 [==============================] - 204s 863ms/step - loss: 0.1172 - val_loss: 0.2010\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.18897\n",
      "Epoch 120/200\n",
      "236/236 [==============================] - 203s 860ms/step - loss: 0.1172 - val_loss: 0.1928\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.18897\n",
      "Epoch 121/200\n",
      "236/236 [==============================] - 203s 860ms/step - loss: 0.1123 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.18897 to 0.18469, saving model to folds3.hdf5\n",
      "Epoch 122/200\n",
      "236/236 [==============================] - 202s 858ms/step - loss: 0.1078 - val_loss: 0.1880\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18469\n",
      "Epoch 123/200\n",
      "236/236 [==============================] - 202s 856ms/step - loss: 0.1076 - val_loss: 0.1850\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.18469\n",
      "Epoch 124/200\n",
      "236/236 [==============================] - 203s 861ms/step - loss: 0.1069 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18469\n",
      "Epoch 125/200\n",
      "236/236 [==============================] - 211s 893ms/step - loss: 0.1063 - val_loss: 0.1843\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.18469 to 0.18428, saving model to folds3.hdf5\n",
      "Epoch 126/200\n",
      "236/236 [==============================] - 209s 885ms/step - loss: 0.1045 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.18428\n",
      "Epoch 127/200\n",
      "236/236 [==============================] - 207s 879ms/step - loss: 0.1034 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18428\n",
      "Epoch 128/200\n",
      "236/236 [==============================] - 207s 879ms/step - loss: 0.1046 - val_loss: 0.1883\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.18428\n",
      "Epoch 129/200\n",
      "236/236 [==============================] - 209s 885ms/step - loss: 0.1091 - val_loss: 0.1853\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.18428\n",
      "Epoch 130/200\n",
      "236/236 [==============================] - 207s 878ms/step - loss: 0.1067 - val_loss: 0.1949\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.18428\n",
      "Epoch 131/200\n",
      "236/236 [==============================] - 209s 884ms/step - loss: 0.1088 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.18428\n",
      "Epoch 132/200\n",
      "236/236 [==============================] - 207s 877ms/step - loss: 0.1023 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.18428 to 0.18371, saving model to folds3.hdf5\n",
      "Epoch 133/200\n",
      "236/236 [==============================] - 207s 878ms/step - loss: 0.1059 - val_loss: 0.1845\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.18371\n",
      "Epoch 134/200\n",
      "236/236 [==============================] - 207s 877ms/step - loss: 0.1016 - val_loss: 0.1843\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.18371\n",
      "Epoch 135/200\n",
      "236/236 [==============================] - 230s 975ms/step - loss: 0.1020 - val_loss: 0.1851\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.18371\n",
      "Epoch 136/200\n",
      "236/236 [==============================] - 207s 876ms/step - loss: 0.1004 - val_loss: 0.1849\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.18371\n",
      "Epoch 137/200\n",
      "236/236 [==============================] - 208s 880ms/step - loss: 0.1696 - val_loss: 0.2009\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.18371\n",
      "Epoch 138/200\n",
      "236/236 [==============================] - 202s 856ms/step - loss: 0.1208 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.18371\n",
      "Epoch 139/200\n",
      "236/236 [==============================] - 193s 816ms/step - loss: 0.1056 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.18371 to 0.18321, saving model to folds3.hdf5\n",
      "Epoch 140/200\n",
      "236/236 [==============================] - 190s 805ms/step - loss: 0.1172 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.18321\n",
      "Epoch 141/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.1007 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.18321\n",
      "Epoch 142/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0998 - val_loss: 0.2036\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.18321\n",
      "Epoch 143/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.1096 - val_loss: 0.1973\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.18321\n",
      "Epoch 144/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.1303 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.18321\n",
      "Epoch 145/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.1068 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.18321\n",
      "Epoch 146/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.0996 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.18321\n",
      "Epoch 147/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0981 - val_loss: 0.2019\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.18321\n",
      "Epoch 148/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.1182 - val_loss: 0.2330\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.18321\n",
      "Epoch 149/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.1394 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.18321\n",
      "Epoch 150/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.1107 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.18321 to 0.17906, saving model to folds3.hdf5\n",
      "Epoch 151/200\n",
      "236/236 [==============================] - 188s 799ms/step - loss: 0.0902 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.17906 to 0.17689, saving model to folds3.hdf5\n",
      "Epoch 152/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0860 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.17689 to 0.17615, saving model to folds3.hdf5\n",
      "Epoch 153/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0877 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17615\n",
      "Epoch 154/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0834 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.17615 to 0.17370, saving model to folds3.hdf5\n",
      "Epoch 155/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0815 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17370\n",
      "Epoch 156/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0821 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17370\n",
      "Epoch 157/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0810 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17370\n",
      "Epoch 158/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0795 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.17370 to 0.17369, saving model to folds3.hdf5\n",
      "Epoch 159/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0816 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17369\n",
      "Epoch 160/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.0814 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17369\n",
      "Epoch 161/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0807 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17369\n",
      "Epoch 162/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0788 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17369\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0777 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17369\n",
      "Epoch 164/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0834 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17369\n",
      "Epoch 165/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0745 - val_loss: 0.1707\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.17369 to 0.17073, saving model to folds3.hdf5\n",
      "Epoch 166/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0774 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17073\n",
      "Epoch 167/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0735 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17073\n",
      "Epoch 168/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0717 - val_loss: 0.1712\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17073\n",
      "Epoch 169/200\n",
      "236/236 [==============================] - 187s 792ms/step - loss: 0.0704 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17073\n",
      "Epoch 170/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0727 - val_loss: 0.1707\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.17073 to 0.17068, saving model to folds3.hdf5\n",
      "Epoch 171/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0699 - val_loss: 0.1699\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.17068 to 0.16993, saving model to folds3.hdf5\n",
      "Epoch 172/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0694 - val_loss: 0.1705\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.16993\n",
      "Epoch 173/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0693 - val_loss: 0.1701\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.16993\n",
      "Epoch 174/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0690 - val_loss: 0.1703\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.16993\n",
      "Epoch 175/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.0684 - val_loss: 0.1705\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.16993\n",
      "Epoch 176/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0682 - val_loss: 0.1704\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.16993\n",
      "Epoch 177/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0694 - val_loss: 0.1706\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.16993\n",
      "Epoch 178/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0676 - val_loss: 0.1693\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.16993 to 0.16927, saving model to folds3.hdf5\n",
      "Epoch 179/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.0681 - val_loss: 0.1703\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.16927\n",
      "Epoch 180/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0681 - val_loss: 0.1697\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.16927\n",
      "Epoch 181/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0680 - val_loss: 0.1692\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.16927 to 0.16919, saving model to folds3.hdf5\n",
      "Epoch 182/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0665 - val_loss: 0.1694\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.16919\n",
      "Epoch 183/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0663 - val_loss: 0.1696\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.16919\n",
      "Epoch 184/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0666 - val_loss: 0.1694\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.16919\n",
      "Epoch 185/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0659 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.16919\n",
      "Epoch 186/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.0661 - val_loss: 0.1691\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.16919 to 0.16911, saving model to folds3.hdf5\n",
      "Epoch 187/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0662 - val_loss: 0.1693\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.16911\n",
      "Epoch 188/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0655 - val_loss: 0.1697\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.16911\n",
      "Epoch 189/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0659 - val_loss: 0.1694\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.16911\n",
      "Epoch 190/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0650 - val_loss: 0.1693\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.16911\n",
      "Epoch 191/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0651 - val_loss: 0.1694\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.16911\n",
      "Epoch 192/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0656 - val_loss: 0.1694\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.16911\n",
      "Epoch 193/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0648 - val_loss: 0.1695\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.16911\n",
      "Epoch 194/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.0650 - val_loss: 0.1692\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.16911\n",
      "Epoch 195/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.0642 - val_loss: 0.1698\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.16911\n",
      "Epoch 196/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0642 - val_loss: 0.1692\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.16911\n",
      "Epoch 197/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.0621 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.16911 to 0.16794, saving model to folds3.hdf5\n",
      "Epoch 198/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.0607 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.16794 to 0.16789, saving model to folds3.hdf5\n",
      "Epoch 199/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.0599 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.16789 to 0.16787, saving model to folds3.hdf5\n",
      "Epoch 200/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.0597 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.16787 to 0.16786, saving model to folds3.hdf5\n",
      "197/197 - 31s\n",
      "--------------- > Fold 4 < ---------------\n",
      "Epoch 1/200\n",
      "236/236 [==============================] - 197s 800ms/step - loss: 3.9142 - val_loss: 0.9827\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98267, saving model to folds4.hdf5\n",
      "Epoch 2/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.9646 - val_loss: 0.8626\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98267 to 0.86256, saving model to folds4.hdf5\n",
      "Epoch 3/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.8007 - val_loss: 0.8059\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.86256 to 0.80589, saving model to folds4.hdf5\n",
      "Epoch 4/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.7001 - val_loss: 0.6221\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80589 to 0.62212, saving model to folds4.hdf5\n",
      "Epoch 5/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.6427 - val_loss: 0.6053\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62212 to 0.60527, saving model to folds4.hdf5\n",
      "Epoch 6/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.5923 - val_loss: 0.5971\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.60527 to 0.59715, saving model to folds4.hdf5\n",
      "Epoch 7/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5565 - val_loss: 0.5469\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.59715 to 0.54687, saving model to folds4.hdf5\n",
      "Epoch 8/200\n",
      "236/236 [==============================] - 187s 793ms/step - loss: 0.5210 - val_loss: 0.5186\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.54687 to 0.51864, saving model to folds4.hdf5\n",
      "Epoch 9/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5130 - val_loss: 0.5002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 0.51864 to 0.50024, saving model to folds4.hdf5\n",
      "Epoch 10/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4831 - val_loss: 0.5225\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50024\n",
      "Epoch 11/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.4866 - val_loss: 0.4649\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.50024 to 0.46494, saving model to folds4.hdf5\n",
      "Epoch 12/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4532 - val_loss: 0.4464\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.46494 to 0.44644, saving model to folds4.hdf5\n",
      "Epoch 13/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.4316 - val_loss: 0.5863\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44644\n",
      "Epoch 14/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.4879 - val_loss: 0.4475\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44644\n",
      "Epoch 15/200\n",
      "236/236 [==============================] - 190s 804ms/step - loss: 0.4234 - val_loss: 0.4290\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.44644 to 0.42899, saving model to folds4.hdf5\n",
      "Epoch 16/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.4325 - val_loss: 0.4185\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.42899 to 0.41855, saving model to folds4.hdf5\n",
      "Epoch 17/200\n",
      "236/236 [==============================] - 188s 798ms/step - loss: 0.4137 - val_loss: 0.4356\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41855\n",
      "Epoch 18/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.3952 - val_loss: 0.4558\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41855\n",
      "Epoch 19/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.4264 - val_loss: 0.4386\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41855\n",
      "Epoch 20/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.4723 - val_loss: 0.4328\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41855\n",
      "Epoch 21/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.4061 - val_loss: 0.3966\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.41855 to 0.39661, saving model to folds4.hdf5\n",
      "Epoch 22/200\n",
      "236/236 [==============================] - 188s 797ms/step - loss: 0.3765 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.39661 to 0.38344, saving model to folds4.hdf5\n",
      "Epoch 23/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.3725 - val_loss: 0.3855\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.38344\n",
      "Epoch 24/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3536 - val_loss: 0.4796\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.38344\n",
      "Epoch 25/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.5019 - val_loss: 0.3894\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.38344\n",
      "Epoch 26/200\n",
      "236/236 [==============================] - 187s 795ms/step - loss: 0.3744 - val_loss: 0.5672\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.38344\n",
      "Epoch 27/200\n",
      "236/236 [==============================] - 187s 794ms/step - loss: 0.4190 - val_loss: 0.3458\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.38344 to 0.34581, saving model to folds4.hdf5\n",
      "Epoch 28/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3380 - val_loss: 0.3593\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.34581\n",
      "Epoch 29/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.3769 - val_loss: 0.3772\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.34581\n",
      "Epoch 30/200\n",
      "236/236 [==============================] - 188s 795ms/step - loss: 0.3237 - val_loss: 0.3491\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.34581\n",
      "Epoch 31/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3561 - val_loss: 0.3415\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.34581 to 0.34154, saving model to folds4.hdf5\n",
      "Epoch 32/200\n",
      "236/236 [==============================] - 188s 796ms/step - loss: 0.3748 - val_loss: 0.3647\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.34154\n",
      "Epoch 33/200\n",
      "236/236 [==============================] - 189s 799ms/step - loss: 0.3531 - val_loss: 0.3738\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.34154\n",
      "Epoch 34/200\n",
      "236/236 [==============================] - 189s 800ms/step - loss: 0.3691 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.34154\n",
      "Epoch 35/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.3161 - val_loss: 0.3148\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.34154 to 0.31477, saving model to folds4.hdf5\n",
      "Epoch 36/200\n",
      "236/236 [==============================] - 189s 802ms/step - loss: 0.2941 - val_loss: 0.4000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.31477\n",
      "Epoch 37/200\n",
      "236/236 [==============================] - 193s 819ms/step - loss: 0.3675 - val_loss: 0.3941\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.31477\n",
      "Epoch 38/200\n",
      "236/236 [==============================] - 201s 851ms/step - loss: 0.3127 - val_loss: 0.4051\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31477\n",
      "Epoch 39/200\n",
      "236/236 [==============================] - 206s 873ms/step - loss: 0.3124 - val_loss: 0.3356\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.31477\n",
      "Epoch 40/200\n",
      "236/236 [==============================] - 206s 874ms/step - loss: 0.2852 - val_loss: 0.3071\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.31477 to 0.30708, saving model to folds4.hdf5\n",
      "Epoch 41/200\n",
      "236/236 [==============================] - 205s 869ms/step - loss: 0.2934 - val_loss: 0.3499\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.30708\n",
      "Epoch 42/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.2996 - val_loss: 0.3254\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.30708\n",
      "Epoch 43/200\n",
      "236/236 [==============================] - 200s 848ms/step - loss: 0.2887 - val_loss: 0.2836\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.30708 to 0.28356, saving model to folds4.hdf5\n",
      "Epoch 44/200\n",
      "236/236 [==============================] - 206s 871ms/step - loss: 0.2594 - val_loss: 0.2824\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.28356 to 0.28238, saving model to folds4.hdf5\n",
      "Epoch 45/200\n",
      "236/236 [==============================] - 207s 879ms/step - loss: 0.2817 - val_loss: 0.2799\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.28238 to 0.27985, saving model to folds4.hdf5\n",
      "Epoch 46/200\n",
      "236/236 [==============================] - 207s 878ms/step - loss: 0.2743 - val_loss: 0.3973\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.27985\n",
      "Epoch 47/200\n",
      "236/236 [==============================] - 193s 817ms/step - loss: 0.3232 - val_loss: 0.3359\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.27985\n",
      "Epoch 48/200\n",
      "236/236 [==============================] - 196s 832ms/step - loss: 0.2928 - val_loss: 0.3233\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.27985\n",
      "Epoch 49/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.2809 - val_loss: 0.2916\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27985\n",
      "Epoch 50/200\n",
      "236/236 [==============================] - 203s 862ms/step - loss: 0.2586 - val_loss: 0.3770\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27985\n",
      "Epoch 51/200\n",
      "236/236 [==============================] - 190s 803ms/step - loss: 0.2757 - val_loss: 0.3049\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.27985\n",
      "Epoch 52/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.3604 - val_loss: 0.3002\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.27985\n",
      "Epoch 53/200\n",
      "236/236 [==============================] - 195s 828ms/step - loss: 0.2593 - val_loss: 0.2959\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.27985\n",
      "Epoch 54/200\n",
      "236/236 [==============================] - 196s 830ms/step - loss: 0.2668 - val_loss: 0.2749\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.27985 to 0.27492, saving model to folds4.hdf5\n",
      "Epoch 55/200\n",
      " 76/236 [========>.....................] - ETA: 2:03 - loss: 0.2432"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_303544/1608661087.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             )\n\u001b[0;32m---> 67\u001b[0;31m             history = model.fit(X_train, np.append(y_train, u_out_train, axis=1),\n\u001b[0m\u001b[1;32m     68\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_out_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                 epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def GBVPP_loss(y_true, y_pred, cols=80):\n",
    "    u_out = y_true[:, cols:]\n",
    "    y = y_true[:, :cols]\n",
    "    w = 1 - u_out\n",
    "    mae = w * tf.abs(y - y_pred)\n",
    "    return tf.reduce_sum(mae, axis=-1) / tf.reduce_sum(w, axis=-1)\n",
    "\n",
    "#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "#tf.config.experimental_connect_to_cluster(tpu)\n",
    "#tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "strategy = tf.distribute.get_strategy()  # gpu\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "with strategy.scope():\n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
    "    test_preds = []\n",
    "    history_arr = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "        K.clear_session()\n",
    "        \n",
    "        print('-'*15, '>', f'Fold {FOLDS[fold]}', '<', '-'*15)\n",
    "        X_train, X_valid = train[train_idx], train[test_idx]\n",
    "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
    "        u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx] \n",
    "        \n",
    "        checkpoint_filepath = f\"folds{FOLDS[fold]}.hdf5\"\n",
    "        if TRAIN_MODEL:\n",
    "            inputs = keras.layers.Input(shape=train.shape[-2:])\n",
    "            \n",
    "            \"\"\"\n",
    "            layer1 = keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True, dropout=0.0))(inputs)\n",
    "            layer2 = keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True, dropout=0.0))(layer1)\n",
    "            layer3 = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True, dropout=0.0))(layer2)\n",
    "            layer4 = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout=0.0))(layer3)\n",
    "            layer5 = keras.layers.Dense(128, activation='selu')(layer4)\n",
    "            \"\"\"\n",
    "            \n",
    "            #\"\"\"\n",
    "            #layer1 = keras.layers.Bidirectional(keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(800, input_shape=train.shape[-2:], dropout=0.0),\n",
    "            #                                                     return_sequences=True))(inputs)\n",
    "            layer2 = keras.layers.Bidirectional(keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(400, input_shape=train.shape[-2:], dropout=0.0),\n",
    "                                                                 return_sequences=True))(inputs)\n",
    "            layer3 = keras.layers.Bidirectional(keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(200, input_shape=train.shape[-2:], dropout=0.0),\n",
    "                                                                 return_sequences=True))(layer2)\n",
    "            layer4 = keras.layers.Bidirectional(keras.layers.RNN(tfa.rnn.LayerNormLSTMCell(100, input_shape=train.shape[-2:], dropout=0.0),\n",
    "                                                                 return_sequences=True))(layer3)\n",
    "            layer5 = keras.layers.Dense(100, activation='selu')(layer4)\n",
    "            #\"\"\"\n",
    "            \n",
    "            preds = keras.layers.Dense(1)(layer5)\n",
    "            model = keras.Model(inputs=inputs, outputs=preds)\n",
    "            \n",
    "            #model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "            model.compile(optimizer=\"adam\", loss=GBVPP_loss)\n",
    "\n",
    "            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "            sv = keras.callbacks.ModelCheckpoint(\n",
    "                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "                options=None\n",
    "            )\n",
    "            history = model.fit(X_train, np.append(y_train, u_out_train, axis=1),\n",
    "                                validation_data=(X_valid, np.append(y_valid, u_out_valid, axis=1)),\n",
    "                                epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
    "        else:\n",
    "            model = keras.models.load_model('./'+checkpoint_filepath)\n",
    "            \n",
    "        test_preds.append(model.predict(test, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n",
    "        history_arr.append(history)\n",
    "        del model, X_train, X_valid; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2485441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE FOLDS WITH MEAN\n",
    "df_submission[\"pressure\"] = sum(test_preds) / len(test_preds)\n",
    "df_submission.to_csv('submission_mean_LB157.csv', index=False)\n",
    "\n",
    "# ENSEMBLE FOLDS WITH MEDIAN\n",
    "df_submission[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)\n",
    "df_submission.to_csv('submission_median_LB155.csv', index=False)\n",
    "\n",
    "# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
    "df_submission[\"pressure\"] = np.round( (df_submission.pressure - PRESSURE_MIN) / PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n",
    "df_submission.pressure = np.clip(df_submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
    "df_submission.to_csv('submission_median_round_LB153.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e28cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
