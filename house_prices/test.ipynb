{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50e4cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "865c1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def pre_process(df):\n",
    "    # removing nans\n",
    "    cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\",\n",
    "            \"GarageCond\", \"BsmtExposure\", \"BsmtFinType2\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\", \"MasVnrType\",\n",
    "            \"Electrical\", \"KitchenQual\", \"Functional\", \"SaleType\", \"MSZoning\", \"Utilities\", \"Exterior1st\",\n",
    "            \"Exterior2nd\"]\n",
    "    df.loc[:, cols] = df.loc[:, cols].fillna(\"zzz\")\n",
    "    cols = [\"GarageYrBlt\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"BsmtFullBath\", \n",
    "            \"BsmtHalfBath\", \"GarageCars\", \"GarageArea\", \"TotalBsmtSF\"]\n",
    "    df.loc[:, cols] = df.loc[:, cols].fillna(0)\n",
    "    df.loc[:, \"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    assert df.isna().sum().max() < len(df) / 100\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # features reworking\n",
    "    df.loc[:, \"YearBuilt\"] = (df.loc[:, \"YearBuilt\"] - df.loc[:, \"YearBuilt\"].min()).astype(float)\n",
    "    df.loc[:, \"YearRemodAdd\"] = (df.loc[:, \"YearRemodAdd\"] - df.loc[:, \"YearRemodAdd\"].min()).astype(float)\n",
    "    df.loc[:, \"WhenSold\"] = (df.loc[:, \"YrSold\"] - df.loc[:, \"YrSold\"].min()).astype(float)\n",
    "    df.loc[:, \"WhenSold\"] = df.loc[:, \"WhenSold\"] + df.loc[:, \"MoSold\"].astype(float) / 12\n",
    "    df = df.drop([\"YrSold\", \"MoSold\"], axis=1)\n",
    "    \n",
    "    # new features\n",
    "    df.loc[:, \"totalSf\"] = df.loc[:, \"1stFlrSF\"] + df.loc[:, \"2ndFlrSF\"]\n",
    "    \n",
    "    # skewed features\n",
    "    numeric_feats = df.dtypes[df.dtypes != \"object\"].index.tolist()\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    skewed_feats = skewed_feats.index.tolist()\n",
    "    df.loc[:, skewed_feats] = np.log1p(df.loc[:, skewed_feats])\n",
    "    \n",
    "    # rescaling\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    cols = df.dtypes[df.dtypes == float].index.tolist()\n",
    "    cols += [\"MiscVal\", \"LotArea\", \"totalSf\", \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\", \"WoodDeckSF\", \"OpenPorchSF\"]\n",
    "    scaler.fit(df.loc[:, cols])\n",
    "    df.loc[:, cols] = scaler.transform(df.loc[:, cols])\n",
    "    \"\"\"\n",
    "    \n",
    "    # dummies\n",
    "    cols = [\"MSSubClass\"]\n",
    "    df[cols] = df[cols].astype(object)\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    \n",
    "    # objects to int\n",
    "    \"\"\"\n",
    "    cols = df.dtypes[df.dtypes == object].index.tolist()\n",
    "    cols += [\"MSSubClass\"]\n",
    "    for c in cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df.loc[:, c].values) \n",
    "        df.loc[:, c] = le.transform(list(df.loc[:, c].values))\n",
    "    \"\"\"\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "448e02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df = pre_process(pd.concat([df_train.drop(\"SalePrice\", axis=1), df_test]))\n",
    "\n",
    "x = df.loc[df[\"Id\"].isin(df_train[\"Id\"]), :]\n",
    "x = x.drop(\"Id\", axis=1).to_numpy()\n",
    "y = np.log1p(df_train[\"SalePrice\"].loc[df_train[\"Id\"].isin(df[\"Id\"])]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc1a11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linreg (0.8103072468249412, 0.0772716587343406)\n",
      "ridge (0.8690616054847984, 0.04824590857246475)\n",
      "lasso (0.8867335525404185, 0.03831551869610051)\n",
      "elasticnet (0.8920999566999918, 0.030327518342146108)\n",
      "krr (0.7954720746592994, 0.08859825431003987)\n",
      "svm (0.8702718609733635, 0.04257195027352872)\n",
      "tree (0.7392306223819352, 0.03412397305990792)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "testing simple models\n",
    "\"\"\"\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def get_score(model, x, y, n_folds=5):\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True)\n",
    "    scores = list()\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        train_x, test_x = x[train_index], x[test_index]\n",
    "        train_y, test_y = y[train_index], y[test_index]\n",
    "        model.fit(train_x, train_y)\n",
    "        scores.append(r2_score(test_y, model.predict(test_x)))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = make_pipeline(RobustScaler(), LinearRegression())\n",
    "print(\"linreg\", get_score(linreg, x, y))\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = make_pipeline(RobustScaler(), Ridge(alpha=0.1, random_state=0))\n",
    "print(\"ridge\", get_score(ridge, x, y))\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(random_state=0))\n",
    "print(\"lasso\", get_score(lasso, x, y))\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNet(random_state=0))\n",
    "print(\"elasticnet\", get_score(lasso, x, y))\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "print(\"krr\", get_score(krr, x, y))\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "svm = make_pipeline(RobustScaler(), SVR(gamma=1e-2))\n",
    "print(\"svm\", get_score(svm, x, y))\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "print(\"tree\", get_score(DecisionTreeRegressor(random_state=0, max_depth=6), x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26ceff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gboost (0.8932432133431281, 0.038942416408498264)\n",
      "[00:47:50] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:47:52] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:47:54] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:47:57] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:48:00] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "xgboost (0.9035255870104459, 0.015188122272048995)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "lgboost (0.9049274064903903, 0.02045469155394383)\n",
      "forest (0.8494169345715432, 0.02164534135993681)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "print(\"gboost\", get_score(GBoost, x, y))\n",
    "\n",
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "print(\"xgboost\", get_score(model_xgb, x, y))\n",
    "\n",
    "import lightgbm as lgb\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "print(\"lgboost\", get_score(model_lgb, x, y))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "print(\"forest\", get_score(RandomForestRegressor(random_state=0, max_depth=6), x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "099f1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuralnet (0.45993950679231654, 0.10501618285501939)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "neuralnet = make_pipeline(RobustScaler(), MLPRegressor(random_state=0, max_iter=10000,\n",
    "                                                       hidden_layer_sizes=[20, 20, 10], solver='adam'))\n",
    "print(\"neuralnet\", get_score(neuralnet, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8514773701068874, 0.017668851941946704)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "averaged model\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, x, y):\n",
    "        self.models_ = [clone(m) for m in self.models]\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    # now we do the predictions for cloned models and average them\n",
    "    def predict(self, x):\n",
    "        predictions = np.column_stack([model.predict(x) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)\n",
    "    \n",
    "averaged_models = AveragingModels(models=(elasticnet, GBoost, svm, ridge))\n",
    "print(get_score(averaged_models, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "736bc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9030899567861812\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ensemble model\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    def fit(self, x, y):\n",
    "        self.base_models_ = [list() for m in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((x.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, test_index in kfold.split(x, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(x[train_index], y[train_index])\n",
    "                yp = instance.predict(x[test_index])\n",
    "                out_of_fold_predictions[test_index, i] = yp\n",
    "                \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, x):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(x) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    \n",
    "model = StackingAveragedModels(base_models=(elasticnet, GBoost, svm, ridge), meta_model=lasso)\n",
    "l = 2 * len(x) // 3\n",
    "model.fit(x[:l], y[:l])\n",
    "print(r2_score(y[l:], model.predict(x[l:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15f4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:15] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "getting final prediction\n",
    "\"\"\"\n",
    "model.fit(x, y)\n",
    "model_xgb.fit(x, y)\n",
    "model_lgb.fit(x, y)\n",
    "\n",
    "x_to_predict = df.loc[df[\"Id\"].isin(df_test[\"Id\"]), :]\n",
    "indices = x_to_predict[\"Id\"]\n",
    "x_to_predict = x_to_predict.drop(\"Id\", axis=1).to_numpy()\n",
    "\n",
    "yp_model = model.predict(x_to_predict)\n",
    "yp_xgb = model_xgb.predict(x_to_predict)\n",
    "yp_lgb = model_lgb.predict(x_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "481ed5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = 0.7 * yp_model + 0.15 * yp_xgb + 0.15 * yp_lgb\n",
    "yp = np.expm1(yp)\n",
    "\n",
    "dfp = pd.DataFrame({'Id': indices, 'SalePrice': yp})\n",
    "dfp = pd.merge(df_test[\"Id\"], dfp, how='left')\n",
    "dfp[\"SalePrice\"] = dfp[\"SalePrice\"].fillna(dfp[\"SalePrice\"].median())\n",
    "dfp.reset_index(drop=True)\n",
    "dfp.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "819610ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041360948112087\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn stack\n",
    "\"\"\"\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "base_learners = [('elasticnet', elasticnet), ('GBoost', GBoost), ('svm', svm), ('ridge', ridge)]\n",
    "final_stack = StackingRegressor(estimators=base_learners, final_estimator=lasso, passthrough=True,\n",
    "                                verbose=False, cv=5)\n",
    "l = 2 * len(x) // 3\n",
    "final_stack.fit(x[:l], y[:l])\n",
    "print(r2_score(y[l:], final_stack.predict(x[l:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "191ec0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.038104052418002965, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06280665254628204, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06143598882581358, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05849489025958832, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05536269250577419, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05165106592556867, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04982199464908277, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04948405654388921, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04710418939702521, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04380590247715688, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0415988225731212, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/kirill/opt/miniconda/envs/kaggle/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:526: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04073424833082129, tolerance: 0.01900808140370263\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "final_stack.fit(x, y)\n",
    "\n",
    "x_to_predict = df.loc[df[\"Id\"].isin(df_test[\"Id\"]), :]\n",
    "indices = x_to_predict[\"Id\"]\n",
    "x_to_predict = x_to_predict.drop(\"Id\", axis=1).to_numpy()\n",
    "\n",
    "yp_stack = final_stack.predict(x_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d186bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = np.expm1(yp_stack)\n",
    "\n",
    "dfp = pd.DataFrame({'Id': indices, 'SalePrice': yp})\n",
    "dfp = pd.merge(df_test[\"Id\"], dfp, how='left')\n",
    "dfp[\"SalePrice\"] = dfp[\"SalePrice\"].fillna(dfp[\"SalePrice\"].median())\n",
    "dfp.reset_index(drop=True)\n",
    "dfp.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
